import os
import json
import re
import uuid
import numpy as np
import faiss
import google.generativeai as genai
from dotenv import load_dotenv
import warnings
import kss
from typing import TypedDict, List, Dict
from langgraph.graph import StateGraph, END
from sklearn.cluster import KMeans
from google.genai import types
from datetime import datetime

import config
from utils import ConsoleLogger, ReportAnalyzer

# pecab ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°œìƒí•˜ëŠ” íŠ¹ì • ëŸ°íƒ€ì„ ê²½ê³ ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.
warnings.filterwarnings(
    "ignore", category=RuntimeWarning, message="overflow encountered in scalar add"
)


class PreprocessorService:
    def __init__(self, status_callback=None):
        """
        ì´ˆê¸°í™” ë©”ì„œë“œ.
        status_callback: GUIì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì½œë°± í•¨ìˆ˜.
        """
        self.status_callback = status_callback
        self._configure_api()

    def _configure_api(self):
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEYë¥¼ .env íŒŒì¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        genai.configure(api_key=api_key)

    def _update_status(self, message):
        """ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°±ì„ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        if self.status_callback:
            self.status_callback(message)
        else:
            print(message)

    def process_files_and_create_db(self, file_paths, db_name):
        """
        ì—¬ëŸ¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ.
        """
        # 1. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ íŒŒì‹± ë° ë°ì´í„° ì²­í¬ ìƒì„±
        all_chunks, report_lines = self._process_markdown_files(file_paths)

        if not all_chunks:
            raise ValueError("ì„ íƒí•œ íŒŒì¼ì—ì„œ ì²˜ë¦¬í•  ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # 2. ë²¡í„° DB ìƒì„±
        num_vectors, faiss_path, json_path = self._create_vector_db(all_chunks, db_name)

        # 3. ìµœì¢… ë³´ê³ 
        report_lines.append("\n--- ë²¡í„° DB ìƒì„± ê²°ê³¼ ---")
        report_lines.append(f"âœ… ì´ {num_vectors}ê°œì˜ ë²¡í„°ë¥¼ ìƒì„±í•˜ì—¬ DB êµ¬ì¶• ì™„ë£Œ.")
        report_lines.append(
            f"âœ… '{os.path.basename(faiss_path)}' ì™€ '{os.path.basename(json_path)}' íŒŒì¼ ì €ì¥ ì™„ë£Œ."
        )

        return "\n".join(report_lines)

    def _process_markdown_files(self, file_paths):
        all_chunks = []
        report_lines = ["--- ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê²°ê³¼ ---"]
        total_files = len(file_paths)

        for i, md_path in enumerate(file_paths):
            self._update_status(f"íŒŒì‹± ì¤‘... ({i+1}/{total_files})")
            try:
                chunks = self._process_single_file(md_path)
                all_chunks.extend(chunks)

                enriched_count = sum(1 for item in chunks if item.get("reference_text"))
                report_lines.append(
                    f"âœ… {os.path.basename(md_path)}:\n"
                    f"    - {len(chunks)}ê°œ ë¬¸ì¥(Chunk) ìƒì„±.\n"
                    f"    - {enriched_count}ê°œì— ì°¸ê³ ë¬¸í—Œ ì—°ê²°."
                )
            except Exception as e:
                report_lines.append(f"âŒ {os.path.basename(md_path)} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return all_chunks, report_lines

    def _parse_references(self, lines):
        references = {}
        is_ref_section = False
        for line in lines:
            if "#### **ì°¸ê³  ìë£Œ**" in line:
                is_ref_section = True
                continue
            if not is_ref_section:
                continue
            match = re.match(r"^\s*(\d+)\s*[-.)]\s*(.*)", line)
            if match:
                ref_num = match.group(1)
                ref_text = match.group(2).strip()
                references[ref_num] = ref_text
        return references

    def _is_valid_ref_num(self, s):
        try:
            first_num = s.replace(",", " ").replace("-", " ").split()[0]
            return int(first_num) < 2000
        except (ValueError, IndexError):
            return False

    def _process_single_file(self, md_path):
        with open(md_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        references_map = self._parse_references(lines)
        chunks = []
        current_headers = []
        stop_parsing = False
        for line in lines:
            line = line.strip()
            if "#### **ì°¸ê³  ìë£Œ**" in line:
                stop_parsing = True
            if stop_parsing or not line:
                continue
            header_match = re.match(r"^(#+)\s+(.*)", line)
            if header_match:
                level = len(header_match.group(1))
                header_text = header_match.group(2).strip()
                while len(current_headers) >= level:
                    current_headers.pop()
                current_headers.append(header_text)
                continue
            if line.startswith(("---", ":", "`")):
                continue
            texts_to_process = []
            if line.startswith("|"):
                cells = line.strip("|").split("|")
                texts_to_process.extend(
                    [cell.strip() for cell in cells if cell.strip()]
                )
            else:
                texts_to_process.append(line)
            for text in texts_to_process:
                try:
                    sentences = kss.split_sentences(text)
                except Exception:
                    sentences = [text]
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    temp_ref_num = None
                    cleaned_sentence = sentence
                    match_end = re.match(r"^(.*?)[\s,.]*([\d,-]+)$", sentence)
                    if match_end:
                        potential_sent = match_end.group(1).strip()
                        potential_ref = match_end.group(2)
                        if potential_sent and self._is_valid_ref_num(potential_ref):
                            cleaned_sentence = potential_sent
                            temp_ref_num = potential_ref
                    if not temp_ref_num:
                        match_start = re.match(r"^([\d,-]+)[\s,.]*(.*)", sentence)
                        if match_start:
                            potential_ref = match_start.group(1)
                            potential_sent = match_start.group(2).strip()
                            if potential_sent and self._is_valid_ref_num(potential_ref):
                                cleaned_sentence = potential_sent
                                temp_ref_num = potential_ref
                    reference_text = None
                    if temp_ref_num:
                        main_ref_num = re.split(r"[, -]", temp_ref_num)[0]
                        if main_ref_num in references_map:
                            reference_text = references_map[main_ref_num]
                    if cleaned_sentence:
                        chunk = {
                            "chunk_id": str(uuid.uuid4()),
                            "file_path": os.path.basename(md_path),
                            "headers": current_headers.copy(),
                            "sentence": cleaned_sentence,
                            "reference_text": reference_text,
                        }
                        chunks.append(chunk)
        return chunks

    def _create_vector_db(self, all_data_items, db_name):
        contents_to_embed = self._prepare_contents(all_data_items)
        embeddings = self._get_embeddings(contents_to_embed)
        faiss_path, json_path = self._build_and_save_faiss_index(
            embeddings, all_data_items, db_name
        )
        return len(embeddings), faiss_path, json_path

    def _prepare_contents(self, data_items):
        contents = []
        for item in data_items:
            sentence = item.get("sentence", "")
            ref_text = item.get("reference_text")

            # ì»¨í…ì¸  ìƒì„±
            if ref_text:
                contents.append(f"ë¬¸ì¥: {sentence}\n\nì¶œì²˜: {ref_text}")
            else:
                contents.append(sentence)

        return contents

    def _get_embeddings(self, contents):
        batch_size = 100
        all_embeddings = []
        total_count = len(contents)

        for i in range(0, total_count, batch_size):
            batch_contents = contents[i : i + batch_size]

            self._update_status(f"ì„ë² ë”© ì¤‘... ({i+len(batch_contents)}/{total_count})")

            result = genai.embed_content(
                model=config.EMBEDDING_MODEL,
                content=batch_contents,
                task_type="RETRIEVAL_DOCUMENT",
            )
            all_embeddings.extend(result["embedding"])

        return np.array(all_embeddings, dtype="float32")

    def _build_and_save_faiss_index(self, embeddings, data_items, db_name):
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)

        faiss_path = f"{db_name}.faiss"
        json_path = f"{db_name}_data.json"

        faiss.write_index(index, faiss_path)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data_items, f, ensure_ascii=False, indent=2)

        return faiss_path, json_path


# LangGraph ìƒíƒœ ì •ì˜
class ReportState(TypedDict):
    topic: str
    outline: str
    report_content: Dict[str, str]
    current_report_text: str
    review_result: dict
    review_history: List[dict]
    review_attempts: int
    formatted_report: str
    final_report_with_refs: str
    progress_message: str


class ReportGeneratorService:
    def __init__(self, progress_queue=None):
        self.progress_queue = progress_queue
        self.index = None
        self.db_data = None
        self.all_vectors = None
        # self.clientëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        self.generative_model = None
        self.embedding_model = None
        self.chunk_id_map = {}
        self.logger = ConsoleLogger()
        self.analyzer = ReportAnalyzer()
        self.models = {}
        self.thinking_budgets = {}
        self.results_folder = None
        self.logs_folder = None
        self.viz_folder = None
        self.graph = self._build_graph()
        self._configure_api()

    def _configure_api(self):
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEYë¥¼ .env íŒŒì¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        genai.configure(api_key=api_key)
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.embedding_model = genai.GenerativeModel(config.EMBEDDING_MODEL)

    def _update_progress(self, message):
        if self.progress_queue:
            self.progress_queue.put({"progress_message": message})
        else:
            print(message)

    def load_vector_db(self, faiss_path):
        db_name = os.path.basename(faiss_path).replace(".faiss", "")
        data_path = os.path.join(os.path.dirname(faiss_path), f"{db_name}_data.json")

        if not os.path.exists(data_path):
            raise FileNotFoundError(
                f"ë§¤ì¹­ë˜ëŠ” ë°ì´í„° íŒŒì¼(.json)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nê²½ë¡œ: {data_path}"
            )

        self.index = faiss.read_index(faiss_path)
        with open(data_path, "r", encoding="utf-8") as f:
            self.db_data = json.load(f)

        self.all_vectors = np.array(
            [self.index.reconstruct(i) for i in range(self.index.ntotal)]
        ).astype("float32")
        self.chunk_id_map = {
            item["chunk_id"]: item
            for item in self.db_data
            if "chunk_id" in item and item.get("reference_text")
        }

        total_chunks = len(self.db_data)
        with_references = len(self.chunk_id_map)

        print(f"\n=== Vector DB ë¡œë“œ ì™„ë£Œ: {os.path.basename(faiss_path)} ===")
        print(f"  - ì „ì²´ ì²­í¬: {total_chunks}")
        print(f"  - ì°¸ê³ ë¬¸í—Œ ìˆëŠ” ì²­í¬: {with_references}")
        print(f"  - ì°¸ê³ ë¬¸í—Œ ë¹„ìœ¨: {with_references/total_chunks*100:.1f}%")
        print("=" * 40)

        return self.index.ntotal

    def run_generation_pipeline(self, topic, mode):
        try:
            if mode == "Production":
                self.models = config.PRODUCTION_MODELS
                self.thinking_budgets = config.PRODUCTION_THINKING_BUDGETS
            else:
                self.models = config.TEST_MODELS
                self.thinking_budgets = config.TEST_THINKING_BUDGETS

            session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.results_folder = f"results_{mode.lower()}_{session_id}"
            self.logs_folder = os.path.join(self.results_folder, "logs")
            self.viz_folder = os.path.join(self.results_folder, "visualizations")

            os.makedirs(self.results_folder, exist_ok=True)
            os.makedirs(self.logs_folder, exist_ok=True)
            os.makedirs(self.viz_folder, exist_ok=True)

            self.logger.start_logging(session_id)
            self.logger.add_log(
                "SYSTEM", "=" * 20 + " ë³´ê³ ì„œ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘ " + "=" * 20
            )

            initial_state = ReportState(
                topic=topic,
                outline="",
                report_content={},
                current_report_text="",
                review_result={},
                review_history=[],
                review_attempts=0,
                formatted_report="",
                final_report_with_refs="",
                progress_message="0/6: íŒŒì´í”„ë¼ì¸ ì‹œì‘...",
            )

            final_state = None
            for event in self.graph.stream(initial_state, {"recursion_limit": 15}):
                node_name = list(event.keys())[0]
                node_output = event[node_name]

                for key, value in node_output.items():
                    if isinstance(value, str) and len(value) > 300:
                        self.logger.add_log("DEBUG", f"  - {key}: (ë‚´ìš©ì´ ê¸¸ì–´ ìƒëµë¨)")
                    elif key not in ["client", "root"]:
                        self.logger.add_log("DEBUG", f"  - {key}: {value}")

                if (
                    "progress_message" in node_output
                    and node_output["progress_message"]
                ):
                    self._update_progress(node_output["progress_message"])

                final_state = node_output

            self.logger.add_log(
                "SYSTEM", "=" * 20 + " ë³´ê³ ì„œ ìƒì„± íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ " + "=" * 20
            )

            if not final_state:
                raise Exception("ê·¸ë˜í”„ ì‹¤í–‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            final_report_with_refs = final_state.get(
                "final_report_with_refs", "ì˜¤ë¥˜: ìµœì¢… ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

            now = datetime.now()
            date_str = now.strftime("%Y%m%d_%H%M%S")
            report_filename = os.path.join(
                self.results_folder, f"final_report_{date_str}.md"
            )
            self.logger.add_log(
                "INFO", f"[6/6] ë³´ê³ ì„œ íŒŒì¼ ì €ì¥ ì¤‘... -> '{report_filename}'"
            )

            with open(report_filename, "w", encoding="utf-8") as f:
                f.write(final_report_with_refs)

            full_log_path, node_log_paths = self.logger.save_logs(self.logs_folder)

            dashboard_path = self.analyzer.create_visualization_dashboard(
                self.logger, final_state, self.viz_folder
            )

            self.logger.add_log(
                "SUCCESS",
                f"ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ í´ë”: {self.results_folder}",
            )

            if self.progress_queue:
                self.progress_queue.put(
                    {
                        "final_report": final_report_with_refs,
                        "dashboard_path": dashboard_path,
                        "log_path": full_log_path,
                    }
                )

        except Exception as e:
            import traceback

            traceback.print_exc()
            self.logger.add_log("ERROR", f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.progress_queue:
                self.progress_queue.put({"error": f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"})
        finally:
            if self.progress_queue:
                self.progress_queue.put({"generation_done": True})

    def _get_model_for_task(self, task_name):
        model_name = self.models.get(task_name, "gemini-1.5-flash-latest")
        return genai.GenerativeModel(model_name)

    def _get_generation_config(self, task_name):
        budget = self.thinking_budgets.get(task_name)
        if budget is not None:
            return {"thinking_config": {"thinking_budget": budget}}
        return None

    def _search_similar_documents(self, query, k=10):
        query_embedding = genai.embed_content(
            model=config.EMBEDDING_MODEL,  # embed_contentëŠ” ëª¨ë¸ ì´ë¦„ì„ ì§ì ‘ ë°›ìŒ
            content=query,
            task_type="RETRIEVAL_QUERY",
        )["embedding"]
        candidate_k = min(k * 3, len(self.db_data))
        distances, indices = self.index.search(
            np.array([query_embedding], dtype="float32"), candidate_k
        )
        candidates = [self.db_data[i] for i in indices[0]]
        with_refs = [c for c in candidates if c.get("reference_text")]
        without_refs = [c for c in candidates if not c.get("reference_text")]
        result = with_refs[:k] + without_refs[: max(0, k - len(with_refs))]
        return result[:k]

    def _extract_key_themes(self):
        num_clusters = min(config.NUM_CLUSTERS_FOR_OUTLINE, self.index.ntotal)
        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        kmeans.fit(self.all_vectors)
        representative_indices = []
        for i in range(num_clusters):
            cluster_indices = np.where(kmeans.labels_ == i)[0]
            if len(cluster_indices) > 0:
                cluster_center = kmeans.cluster_centers_[i]
                distances = faiss.pairwise_distances(
                    cluster_center.reshape(1, -1), self.all_vectors[cluster_indices]
                )
                closest_in_cluster = np.argmin(distances)
                representative_indices.append(cluster_indices[closest_in_cluster])
        return [self.db_data[i] for i in representative_indices]

    def _create_search_queries_for_section(self, header, topic, outline):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì„¹ì…˜ì— ëŒ€í•œ ë‹¤ì–‘í•˜ê³  êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.logger.add_log("DEBUG", f"'{header}' ì„¹ì…˜ì— ëŒ€í•œ ê²€ìƒ‰ì–´ ìƒì„± ì¤‘...")

        prompt = f"""
        ë‹¹ì‹ ì€ íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•´, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•  ê²€ìƒ‰ì–´ë¥¼ ë§Œë“œëŠ” ì „ë¬¸ ì—°êµ¬ì›ì…ë‹ˆë‹¤.
        ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, '{header}' ì„¹ì…˜ì„ ì‘ì„±í•˜ëŠ” ë° í•„ìš”í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì–´ {config.NUM_SEARCH_QUERIES}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

        --- ì „ì²´ ë³´ê³ ì„œ ì£¼ì œ ---
        {topic}

        --- ì „ì²´ ë³´ê³ ì„œ ê°œìš” ---
        {outline}
        --- ë ---

        **ì§€ì‹œì‚¬í•­:**
        1. **ë‹¤ì–‘ì„±:** ìƒì„±ë˜ëŠ” ê²€ìƒ‰ì–´ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ê³¼ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ë™ì¼í•œ ê²€ìƒ‰ì–´ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
        2. **êµ¬ì²´ì„±:** "ACPì˜ ì—­ì‚¬"ì™€ ê°™ì€ í¬ê´„ì ì¸ ê²€ìƒ‰ì–´ ëŒ€ì‹ , "ë²¨ê¸°ì— ACP ì œë„ì˜ ì´ˆê¸° íŒë¡€" ë˜ëŠ” "2022ë…„ ACP ë²•ì œí™”ì˜ ì£¼ìš” ë™ì¸"ê³¼ ê°™ì´ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“œì„¸ìš”.
        3. **ì¶œë ¥ í˜•ì‹:** ê° ê²€ìƒ‰ì–´ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬, ì˜¤ì§ ê²€ìƒ‰ì–´ ëª©ë¡ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """
        model = self._get_model_for_task("query_generation")
        response = model.generate_content(prompt)
        queries = [q.strip() for q in response.text.split("\n") if q.strip()]

        self.logger.add_log("DEBUG", f"ìƒì„±ëœ ê²€ìƒ‰ì–´: {queries}")
        return queries

    def _generate_outline_logic(self, topic):
        key_theme_contexts = self._extract_key_themes()
        topic_specific_contexts = self._search_similar_documents(
            topic, k=config.K_FOR_TOPIC_SEARCH
        )
        combined_contexts = {
            item["sentence"]: item
            for item in key_theme_contexts + topic_specific_contexts
        }.values()
        context_str = "\n\n---\n\n".join(
            [
                f"ë¬¸ì„œ: {c['file_path']}\nëª©ì°¨: {' > '.join(c['headers'])}\në¬¸ì¥: {c['sentence']}"
                for c in combined_contexts
            ]
        )
        prompt = config.OUTLINE_PROMPT_TEMPLATE.format(
            report_purpose=config.REPORT_PURPOSE,
            topic=topic,
            guideline=config.EDITORIAL_GUIDELINE,
            context_str=context_str,
        )
        model = self._get_model_for_task("outline_generation")
        generation_config = self._get_generation_config("outline_generation")
        response = model.generate_content(
            contents=prompt, generation_config=generation_config
        )
        return response.text

    def _generate_single_section(
        self, header, topic, outline, improvement_instructions=""
    ):
        # ì´ í•¨ìˆ˜ëŠ” `node_generate_draft`ì™€ `node_regenerate_sections`ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
        self.logger.add_log("DEBUG", f"'{header}' ì„¹ì…˜ ìƒì„± ì‹œì‘...")

        # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•˜ê³  êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ ìƒì„±
        search_queries = self._create_search_queries_for_section(header, topic, outline)

        # 2. ìƒì„±ëœ ê° ê²€ìƒ‰ì–´ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©
        all_contexts = {}
        for query in search_queries:
            contexts = self._search_similar_documents(
                query, k=config.K_FOR_SECTION_DRAFT
            )
            for c in contexts:
                all_contexts[c["chunk_id"]] = c  # chunk_idë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°

        final_contexts = list(all_contexts.values())
        self.logger.add_log(
            "INFO",
            f"'{header}' ì„¹ì…˜ ìœ„í•´ ì´ {len(final_contexts)}ê°œì˜ ê³ ìœ í•œ ì°¸ê³ ìë£Œ ê²€ìƒ‰ ì™„ë£Œ.",
        )

        context_str = "\n\n---\n\n".join(
            [
                f"chunk_id: {c['chunk_id']}\në¬¸ì„œ: {c['file_path']}\nëª©ì°¨: {' > '.join(c['headers'])}\në¬¸ì¥: {c['sentence']}\nì°¸ê³ ë¬¸í—Œ: {c.get('reference_text', 'N/A')}"
                for c in final_contexts
            ]
        )

        improvement_prompt = ""
        if improvement_instructions:
            improvement_prompt = f"""
            --- ê°œì„  ì§€ì‹œì‚¬í•­ ---
            ì´ì „ ë²„ì „ì— ëŒ€í•œ í¸ì§‘íŒ€ì˜ ê²€í†  ê²°ê³¼, ë‹¤ìŒê³¼ ê°™ì€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì§€ì‹œì‚¬í•­ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ë‚´ìš©ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            - {improvement_instructions}
            --- ë ---
            """

        prompt = config.DRAFT_SECTION_PROMPT_TEMPLATE.format(
            report_purpose=config.REPORT_PURPOSE,
            topic=topic,
            improvement_prompt=improvement_prompt,
            guideline=config.EDITORIAL_GUIDELINE,
            context_str=context_str,
            header=header,
        )
        model = self._get_model_for_task("draft_generation")
        generation_config = self._get_generation_config("draft_generation")
        response = model.generate_content(
            contents=prompt, generation_config=generation_config
        )
        self.logger.add_log(
            "DEBUG", f"'{header}' ì„¹ì…˜ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(response.text)}ì)"
        )
        return response.text

    def _editorial_review_logic(self, report_text, outline):
        self.logger.add_log("INFO", "í¸ì§‘ì¥ ê²€í†  ë¡œì§ ì‹œì‘...")
        prompt = config.EDITORIAL_REVIEW_PROMPT_TEMPLATE.format(
            report_purpose=config.REPORT_PURPOSE,
            outline=outline,
            guideline=config.EDITORIAL_GUIDELINE,
            report_text=report_text,
        )
        model = self._get_model_for_task("editorial_review")
        generation_config = self._get_generation_config("editorial_review")
        response = model.generate_content(
            contents=prompt, generation_config=generation_config
        )

        try:
            json_text = re.search(
                r"```json\n(.*?)\n```", response.text, re.DOTALL
            ).group(1)
            result = json.loads(json_text)
            self.logger.add_log(
                "SUCCESS",
                "í¸ì§‘ì¥ ê²€í†  ì™„ë£Œ. ê°œì„  í•„ìš” ì„¹ì…˜: "
                + str(len(result.get("sections_to_improve", []))),
            )
            return result
        except (AttributeError, json.JSONDecodeError) as e:
            self.logger.add_log("ERROR", f"í¸ì§‘ì¥ ê²€í†  ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "overall_comment": f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨. ëª¨ë¸ì´ ìœ íš¨í•œ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\nì˜¤ë¥˜: {e}",
                "review_passed": False,
                "sections_to_improve": [],
                "raw_response": response.text,  # ì›ë³¸ ì‘ë‹µ ì¶”ê°€
            }

    def _final_formatting_logic(self, report_text):
        self.logger.add_log("INFO", "ìµœì¢… ì„œì‹ ì •ë¦¬ ë¡œì§ ì‹œì‘...")
        prompt = config.FINAL_FORMATTING_PROMPT_TEMPLATE.format(
            report_text=report_text, guideline=config.EDITORIAL_GUIDELINE
        )
        model = self._get_model_for_task("final_formatting")
        generation_config = self._get_generation_config("final_formatting")
        response = model.generate_content(
            contents=prompt, generation_config=generation_config
        )
        self.logger.add_log("SUCCESS", "ìµœì¢… ì„œì‹ ì •ë¦¬ ì™„ë£Œ.")
        return response.text

    def _finalize_citations_logic(self, final_text):
        self.logger.add_log("INFO", "ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ë¡œì§ ì‹œì‘...")

        # 1. ë³¸ë¬¸ì—ì„œ ëª¨ë“  ê°ì£¼ IDë¥¼ *ìˆœì„œëŒ€ë¡œ* ì°¾ìŠµë‹ˆë‹¤. (set ì‚¬ìš© ì•ˆ í•¨, ^ëŠ” ì„ íƒì‚¬í•­)
        ordered_footnote_ids = re.findall(r"\[\^?([\w-]+)\]", final_text)

        if not ordered_footnote_ids:
            self.logger.add_log("INFO", "ë³¸ë¬¸ì— ì²˜ë¦¬í•  ìœ íš¨í•œ ê°ì£¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return re.sub(r"\[\^?([\w-]+)\]", "", final_text)

        # 2. ê°ì£¼ê°€ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìˆœì„œëŒ€ë¡œ ê³ ìœ í•œ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤.
        unique_references_in_order = []
        seen_refs = set()
        for chunk_id in ordered_footnote_ids:
            chunk_data = self.chunk_id_map.get(chunk_id)
            if chunk_data:
                ref_text = chunk_data.get("reference_text")
                if ref_text and ref_text not in seen_refs:
                    unique_references_in_order.append(ref_text)
                    seen_refs.add(ref_text)

        if not unique_references_in_order:
            self.logger.add_log(
                "INFO", "ë³¸ë¬¸ì— ìœ íš¨í•œ ì°¸ê³ ë¬¸í—Œì´ ì—°ê²°ëœ ê°ì£¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
            return re.sub(r"\[\^?([\w-]+)\]", "", final_text)

        # 3. ë“±ì¥ ìˆœì„œì— ë”°ë¼ ì°¸ê³ ë¬¸í—Œì— ë²ˆí˜¸ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
        ref_to_num_map = {
            ref: i + 1 for i, ref in enumerate(unique_references_in_order)
        }

        # 4. ë³¸ë¬¸ì˜ ê°ì£¼ IDë¥¼ ìˆœì„œì— ë§ëŠ” ë²ˆí˜¸ë¡œ êµì²´í•©ë‹ˆë‹¤.
        def replace_footnote(match):
            chunk_id = match.group(1)
            chunk_data = self.chunk_id_map.get(chunk_id)
            if chunk_data:
                ref_text = chunk_data.get("reference_text")
                if ref_text and ref_text in ref_to_num_map:
                    return f"[{ref_to_num_map[ref_text]}]"
            return ""  # ìœ íš¨í•˜ì§€ ì•Šì€ ê°ì£¼ëŠ” ì œê±°

        final_text_with_numbered_refs = re.sub(
            r"\[\^?([\w-]+)\]", replace_footnote, final_text
        )

        # 5. ë³´ê³ ì„œ ëì— ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        references_section = "\n\n---\n\n## ì°¸ê³ ë¬¸í—Œ\n\n"
        for i, ref in enumerate(unique_references_in_order):
            references_section += f"{i+1}. {ref}\n"

        self.logger.add_log(
            "SUCCESS",
            f"{len(unique_references_in_order)}ê°œì˜ ê³ ìœ  ì°¸ê³ ë¬¸í—Œì„ ìˆœì„œì— ë§ê²Œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.",
        )
        return final_text_with_numbered_refs + references_section

    def node_generate_outline(self, state: ReportState):
        self.logger.set_current_node("generate_outline")
        self.logger.add_log("INFO", "[1/6] ë³´ê³ ì„œ ê°œìš” ìƒì„± ì‹œì‘")
        self._update_progress("[1/6] ë³´ê³ ì„œ ê°œìš” ìƒì„± ì‹œì‘...")
        outline = self._generate_outline_logic(state["topic"])
        self.logger.add_log("SUCCESS", f"ê°œìš” ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(outline)}ì)")
        return {
            "outline": outline,
            "progress_message": "1/6: ê°œìš” ìƒì„± ì™„ë£Œ. ì´ˆì•ˆ ì‘ì„± ì‹œì‘...",
        }

    def node_generate_draft(self, state: ReportState):
        self.logger.set_current_node("generate_draft")
        self.logger.add_log("INFO", "[2/6] ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì‹œì‘")
        self._update_progress("[2/6] ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì‹œì‘...")

        outline = state["outline"]
        topic = state["topic"]

        headers = re.findall(r"^(##+)\s+(.*)", outline, re.MULTILINE)

        report_content = {}
        full_draft_parts = []
        total_headers = len(headers)

        for i, (level, header) in enumerate(headers):
            # ëª©ì°¨ì˜ ë ˆë²¨ì— ë”°ë¼ í—¤ë” ì¶”ê°€
            full_draft_parts.append(f"\n{level} {header}\n")

            progress_msg = f"[2/6] ì´ˆì•ˆ ì‘ì„± ì¤‘... ({i+1}/{total_headers}): {header}"
            self._update_progress(progress_msg)
            self.logger.add_log("INFO", progress_msg)

            section_content = self._generate_single_section(header, topic, outline)
            report_content[header] = section_content
            full_draft_parts.append(section_content)

        full_draft = "\n\n".join(full_draft_parts)

        self.logger.add_log(
            "SUCCESS", f"ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ (ì´ ê¸¸ì´: {len(full_draft)}ì)"
        )
        return {
            "report_content": report_content,
            "current_report_text": full_draft,
            "progress_message": "2/6: ì´ˆì•ˆ ìƒì„± ì™„ë£Œ. í¸ì§‘ì¥ ê²€í†  ì‹œì‘...",
        }

    def node_editorial_review(self, state: ReportState):
        self.logger.set_current_node("editorial_review")
        self.logger.add_log("INFO", "[3/6] í¸ì§‘ì¥ ê²€í†  ë° ê°œì„  ì‘ì—… ì‹œì‘")
        review_attempts = state["review_attempts"] + 1
        self._update_progress(
            f"í¸ì§‘ì¥ ê²€í† ... (ì‹œë„ {review_attempts}/{config.MAX_REVIEW_ATTEMPTS})"
        )

        report_text = state["current_report_text"]

        # 1. ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        char_count = len(report_text)
        word_count = len(report_text.split())
        # ê°ì£¼ ê°œìˆ˜ ê³„ì‚° (ìœ ì—°í•œ ì •ê·œì‹ ì‚¬ìš©)
        ref_count = len(re.findall(r"[(\[]\^?([\w-]+)[)\]]", report_text))
        model_name = self.models.get("editorial_review", "N/A")

        # 2. ê²€í†  ë¡œì§ ì‹¤í–‰
        review_result = self._editorial_review_logic(report_text, state["outline"])

        # 3. ë©”íƒ€ë°ì´í„°ë¥¼ ê²€í†  ê²°ê³¼ì— ì¶”ê°€
        review_result["metadata"] = {
            "model_used": model_name,
            "char_count": char_count,
            "word_count": word_count,
            "reference_count": ref_count,
        }

        self.logger.add_log(
            "INFO",
            f"ê²€í†  ê²°ê³¼: í†µê³¼={review_result.get('review_passed')}, ê°œì„  í•„ìš”={len(review_result.get('sections_to_improve', []))}ê°œ",
        )
        self.logger.add_log(
            "DEBUG", f"ê²€í†  ì´í‰: {review_result.get('overall_comment')}"
        )

        return {
            "review_result": review_result,
            "review_attempts": review_attempts,
            "review_history": state["review_history"] + [review_result],
            "progress_message": "3/6: í¸ì§‘ì¥ ê²€í†  ì™„ë£Œ. í•„ìš”ì‹œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.",
        }

    def node_regenerate_sections(self, state: ReportState):
        self.logger.set_current_node("regenerate_sections")
        sections_to_improve = state["review_result"].get("sections_to_improve", [])
        self.logger.add_log(
            "INFO", f"ë³´ê³ ì„œ ì¼ë¶€ ì¬ì‘ì„± ì‹œì‘ ({len(sections_to_improve)}ê°œ ì„¹ì…˜)"
        )
        self._update_progress(
            f"ë³´ê³ ì„œ ì¼ë¶€ ì¬ì‘ì„± ì¤‘... ({len(sections_to_improve)}ê°œ ì„¹ì…˜)"
        )

        updated_content = state["report_content"].copy()

        for i, section in enumerate(sections_to_improve):
            header = section["section_header"]
            instructions = section["improvement_instruction"]

            progress_msg = f"ì¬ì‘ì„± ì¤‘... ({i+1}/{len(sections_to_improve)}): {header}"
            self._update_progress(progress_msg)
            self.logger.add_log("INFO", progress_msg)
            self.logger.add_log("DEBUG", f"ê°œì„  ì§€ì‹œì‚¬í•­: {instructions}")

            if header in updated_content:
                regenerated_content = self._generate_single_section(
                    header, state["topic"], state["outline"], instructions
                )
                updated_content[header] = regenerated_content
            else:
                self.logger.add_log(
                    "WARN", f"ì¬ì‘ì„±í•  ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {header}"
                )

        # ì¬ì‘ì„±ëœ ë‚´ìš©ìœ¼ë¡œ ì „ì²´ ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        outline = state["outline"]
        headers_in_outline = re.findall(r"^(##+)\s+(.*)", outline, re.MULTILINE)
        new_report_parts = []
        for level, h in headers_in_outline:
            new_report_parts.append(f"\n{level} {h}\n")
            new_report_parts.append(updated_content.get(h, ""))

        new_report_text = "\n\n".join(new_report_parts)

        self.logger.add_log("SUCCESS", "ë³´ê³ ì„œ ì¼ë¶€ ì¬ì‘ì„± ì™„ë£Œ.")
        return {
            "report_content": updated_content,
            "current_report_text": new_report_text,
            "progress_message": "ì¼ë¶€ ì¬ì‘ì„± ì™„ë£Œ. ë‹¤ì‹œ í¸ì§‘ì¥ ê²€í† ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.",
        }

    def node_final_formatting(self, state: ReportState):
        self.logger.set_current_node("final_formatting")
        self.logger.add_log("INFO", "[4/6] ìµœì¢… ì„œì‹ ì •ë¦¬ ì‹œì‘")
        self._update_progress("[4/6] ìµœì¢… ì„œì‹ ì •ë¦¬ ì‹œì‘...")

        original_text = state["current_report_text"]

        # 1. (ë³´í˜¸) ê°€ëŠ¥í•œ ëª¨ë“  í˜•íƒœì˜ ê°ì£¼ë¥¼ 'ìˆœì„œëŒ€ë¡œ' ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.
        # ì •ê·œì‹ì€ [^uuid], [uuid], (uuid), (^uuid) ë“±ì„ ëª¨ë‘ í¬ê´„í•©ë‹ˆë‹¤.
        original_citations_matches = list(
            re.finditer(r"[(\[]\^?([\w-]+)[)\]]", original_text)
        )

        if not original_citations_matches:
            self.logger.add_log(
                "WARN", "ë³´í˜¸í•  ê°ì£¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„œì‹ ì •ë¦¬ë§Œ ì§„í–‰í•©ë‹ˆë‹¤."
            )
            formatted_report = self._final_formatting_logic(original_text)
            return {
                "formatted_report": formatted_report,
                "progress_message": "4/6: ì„œì‹ ì •ë¦¬ ì™„ë£Œ. ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ì‹œì‘...",
            }

        # 2. ë³¸ë¬¸ì˜ ê°ì£¼ë¥¼ [ì°¸ê³ ë¬¸í—Œ:ì¸ë±ìŠ¤] í˜•íƒœë¡œ ìˆœì„œëŒ€ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
        citation_counter = 0

        def protect_placeholder(match):
            nonlocal citation_counter
            placeholder = f"[ì°¸ê³ ë¬¸í—Œ:{citation_counter}]"
            citation_counter += 1
            return placeholder

        protected_text = re.sub(
            r"[(\[]\^?([\w-]+)[)\]]", protect_placeholder, original_text
        )
        self.logger.add_log(
            "DEBUG",
            f"{len(original_citations_matches)}ê°œì˜ ê°ì£¼ë¥¼ ì°¾ì•„ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ êµì²´í–ˆìŠµë‹ˆë‹¤.",
        )

        # 3. ë³´í˜¸ëœ í…ìŠ¤íŠ¸ë¡œ ì„œì‹ ì •ë¦¬ ë¡œì§(LLM í˜¸ì¶œ)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        formatted_protected_text = self._final_formatting_logic(protected_text)

        # 4. (ë³µì›) [ì°¸ê³ ë¬¸í—Œ:ì¸ë±ìŠ¤]ë¥¼ ì›ë˜ì˜ í‘œì¤€ í˜•ì‹ [^uuid]ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.
        restored_text = formatted_protected_text

        # ë³µì›ì„ ìœ„í•´ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ë‹¤ì‹œ ì°¾ìŠµë‹ˆë‹¤.
        temp_placeholders_in_formatted_text = re.findall(
            r"\[ì°¸ê³ ë¬¸í—Œ:(\d+)\]", restored_text
        )

        num_restored = 0
        for index_str in temp_placeholders_in_formatted_text:
            index = int(index_str)
            if index < len(original_citations_matches):
                placeholder_to_replace = f"[ì°¸ê³ ë¬¸í—Œ:{index}]"
                # ì›ë³¸ uuidë¥¼ ê°€ì ¸ì™€ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³µì›
                original_uuid = original_citations_matches[index].group(1)
                standard_citation = f"[^{original_uuid}]"

                # ì¤‘ë³µ êµì²´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 1ë²ˆë§Œ êµì²´
                if placeholder_to_replace in restored_text:
                    restored_text = restored_text.replace(
                        placeholder_to_replace, standard_citation, 1
                    )
                    num_restored += 1

        self.logger.add_log(
            "DEBUG",
            f"{num_restored}/{len(original_citations_matches)}ê°œì˜ ê°ì£¼ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤.",
        )

        self.logger.add_log(
            "SUCCESS", f"ìµœì¢… ì„œì‹ ì •ë¦¬ ì™„ë£Œ (ê¸¸ì´: {len(restored_text)}ì)"
        )
        return {
            "formatted_report": restored_text,
            "progress_message": "4/6: ì„œì‹ ì •ë¦¬ ì™„ë£Œ. ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ì‹œì‘...",
        }

    def node_finalize_citations_and_save_log(self, state: ReportState):
        self.logger.set_current_node("finalize_and_save")
        self.logger.add_log("INFO", "[5/6] ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ë° ë¡œê·¸ ì €ì¥ ì‹œì‘")
        self._update_progress("[5/6] ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ë° ë¡œê·¸ ì €ì¥ ì‹œì‘...")
        final_report_with_refs = self._finalize_citations_logic(
            state["formatted_report"]
        )
        self.logger.add_log(
            "SUCCESS",
            f"ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ (ì°¸ê³ ë¬¸í—Œ í¬í•¨, ê¸¸ì´: {len(final_report_with_refs)}ì)",
        )

        # Save node-specific logs
        for node_name, history in self.logger.node_logs.items():
            log_path = os.path.join(
                self.logs_folder, f"node_{node_name}_log_{self.logger.session_id}.md"
            )
            with open(log_path, "w", encoding="utf-8") as f:
                f.write(f"# Log for node: {node_name}\n\n")
                # historyì˜ ê° ì•„ì´í…œì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ join
                f.write("\n".join(map(str, history)))

        # Save editorial review history
        review_log_path = os.path.join(
            self.results_folder, f"editorial_review_log_{self.logger.session_id}.md"
        )
        with open(review_log_path, "w", encoding="utf-8") as f:
            f.write("# Editorial Review History\n\n")
            for i, review in enumerate(state["review_history"]):
                f.write(f"## Attempt {i+1}\n\n")

                # ë©”íƒ€ë°ì´í„° ê¸°ë¡
                metadata = review.get("metadata", {})
                if metadata:
                    f.write("### Review Metadata\n\n")
                    f.write(
                        f"- **Model Used:** `{metadata.get('model_used', 'N/A')}`\n"
                    )
                    f.write(
                        f"- **Character Count:** {metadata.get('char_count', 0):,}\n"
                    )
                    f.write(f"- **Word Count:** {metadata.get('word_count', 0):,}\n")
                    f.write(
                        f"- **Reference Count:** {metadata.get('reference_count', 0)}\n\n"
                    )

                f.write("### Review Result\n\n")
                f.write(f"**Passed:** {review.get('review_passed')}\n\n")
                f.write(
                    f"**Overall Comment:**\n\n```\n{review.get('overall_comment')}\n```\n\n"
                )

                if review.get("sections_to_improve"):
                    f.write("### Sections to Improve\n\n")
                    for section in review.get("sections_to_improve"):
                        f.write(
                            f"- **{section['section_header']}**: {section['improvement_instruction']}\n"
                        )

                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‘ë‹µ ê¸°ë¡
                if "raw_response" in review:
                    f.write("\n### Raw Model Response (on parsing failure)\n\n")
                    f.write(f"```\n{review['raw_response']}\n```\n\n")

                f.write("\n---\n\n")

        return {
            "final_report_with_refs": final_report_with_refs,
            "progress_message": "5/6: ê°ì£¼ ì²˜ë¦¬ ë° ë¡œê·¸ ì €ì¥ ì™„ë£Œ.",
        }

    def should_continue_review(self, state: ReportState):
        review_result = state["review_result"]
        review_attempts = state["review_attempts"]

        if review_result.get("review_passed"):
            return "end_review"

        if review_attempts >= config.MAX_REVIEW_ATTEMPTS:
            self.logger.add_log(
                "WARN",
                f"ìµœëŒ€ ê²€í†  íšŸìˆ˜({config.MAX_REVIEW_ATTEMPTS})ì— ë„ë‹¬í•˜ì—¬, ê°œì„  ì—†ì´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
            )
            return "end_review"

        if not review_result.get("sections_to_improve"):
            self.logger.add_log(
                "WARN",
                "ê²€í† ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ê°œì„ í•  ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì¬ê²€í† ë¥¼ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.",
            )

        return "regenerate"

    def _build_graph(self):
        workflow = StateGraph(ReportState)
        workflow.add_node("generate_outline", self.node_generate_outline)
        workflow.add_node("generate_draft", self.node_generate_draft)
        workflow.add_node("editorial_review", self.node_editorial_review)
        workflow.add_node("regenerate_sections", self.node_regenerate_sections)
        workflow.add_node("final_formatting", self.node_final_formatting)
        workflow.add_node(
            "finalize_and_save", self.node_finalize_citations_and_save_log
        )
        workflow.set_entry_point("generate_outline")
        workflow.add_edge("generate_outline", "generate_draft")
        workflow.add_edge("generate_draft", "editorial_review")
        workflow.add_conditional_edges(
            "editorial_review",
            self.should_continue_review,
            {"regenerate": "regenerate_sections", "end_review": "final_formatting"},
        )
        workflow.add_edge("regenerate_sections", "editorial_review")
        workflow.add_edge("final_formatting", "finalize_and_save")
        workflow.add_edge("finalize_and_save", END)
        return workflow.compile()
