import os
import json
import re
import uuid
import numpy as np
import faiss
from google import genai
from google.genai import types
from dotenv import load_dotenv
import warnings
import kss
from typing import TypedDict, List, Dict
from langgraph.graph import StateGraph, END
from sklearn.cluster import KMeans
from datetime import datetime

import config
from utils import ConsoleLogger, ReportAnalyzer

# pecab ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°œìƒí•˜ëŠ” íŠ¹ì • ëŸ°íƒ€ì„ ê²½ê³ ë¥¼ ë¬´ì‹œí•©ë‹ˆë‹¤.
warnings.filterwarnings(
    "ignore", category=RuntimeWarning, message="overflow encountered in scalar add"
)


class PreprocessorService:
    def __init__(self, status_callback=None):
        """
        ì´ˆê¸°í™” ë©”ì„œë“œ.
        status_callback: GUIì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì½œë°± í•¨ìˆ˜.
        """
        self.status_callback = status_callback
        self.client = None
        self._configure_api()

    def _configure_api(self):
        load_dotenv()
        if config.GOOGLE_PROJECT_ID:
            # Vertex AI ì‚¬ìš© (ê³µì‹ ë¬¸ì„œ ë°©ì‹)
            self.client = genai.Client(
                vertexai=True, project=config.GOOGLE_PROJECT_ID, location="us-central1"
            )
        else:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError(
                    "GEMINI_API_KEY ë˜ëŠ” GOOGLE_PROJECT_IDë¥¼ .env íŒŒì¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            # Gemini Developer API ì‚¬ìš© (ê³µì‹ ë¬¸ì„œ ë°©ì‹)
            self.client = genai.Client(api_key=api_key)

    def _update_status(self, message):
        """ìƒíƒœ ì—…ë°ì´íŠ¸ ì½œë°±ì„ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        if self.status_callback:
            self.status_callback(message)
        else:
            print(message)

    def process_files_and_create_db(self, file_paths, db_name):
        """
        ì—¬ëŸ¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•˜ëŠ” ë©”ì¸ ë©”ì„œë“œ.
        """
        # 1. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ íŒŒì‹± ë° ë°ì´í„° ì²­í¬ ìƒì„±
        all_chunks, report_lines = self._process_markdown_files(file_paths)

        if not all_chunks:
            raise ValueError("ì„ íƒí•œ íŒŒì¼ì—ì„œ ì²˜ë¦¬í•  ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # 2. ë²¡í„° DB ìƒì„±
        num_vectors, faiss_path, json_path = self._create_vector_db(all_chunks, db_name)

        # 3. ìµœì¢… ë³´ê³ 
        report_lines.append("\n--- ë²¡í„° DB ìƒì„± ê²°ê³¼ ---")
        report_lines.append(f"âœ… ì´ {num_vectors}ê°œì˜ ë²¡í„°ë¥¼ ìƒì„±í•˜ì—¬ DB êµ¬ì¶• ì™„ë£Œ.")
        report_lines.append(
            f"âœ… '{os.path.basename(faiss_path)}' ì™€ '{os.path.basename(json_path)}' íŒŒì¼ ì €ì¥ ì™„ë£Œ."
        )

        return "\n".join(report_lines)

    def _process_markdown_files(self, file_paths):
        all_chunks = []
        report_lines = ["--- ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ê²°ê³¼ ---"]
        total_files = len(file_paths)

        for i, md_path in enumerate(file_paths):
            self._update_status(f"íŒŒì‹± ì¤‘... ({i+1}/{total_files})")
            try:
                chunks = self._process_single_file(md_path)
                all_chunks.extend(chunks)

                enriched_count = sum(1 for item in chunks if item.get("reference_text"))
                report_lines.append(
                    f"âœ… {os.path.basename(md_path)}:\n"
                    f"    - {len(chunks)}ê°œ ë¬¸ì¥(Chunk) ìƒì„±.\n"
                    f"    - {enriched_count}ê°œì— ì°¸ê³ ë¬¸í—Œ ì—°ê²°."
                )
            except Exception as e:
                report_lines.append(f"âŒ {os.path.basename(md_path)} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return all_chunks, report_lines

    def _parse_references(self, lines):
        references = {}
        is_ref_section = False
        for line in lines:
            if "#### **ì°¸ê³  ìë£Œ**" in line:
                is_ref_section = True
                continue
            if not is_ref_section:
                continue
            match = re.match(r"^\s*(\d+)\s*[-.)]\s*(.*)", line)
            if match:
                ref_num = match.group(1)
                ref_text = match.group(2).strip()
                references[ref_num] = ref_text
        return references

    def _is_valid_ref_num(self, s):
        try:
            first_num = s.replace(",", " ").replace("-", " ").split()[0]
            return int(first_num) < 2000
        except (ValueError, IndexError):
            return False

    def _process_single_file(self, md_path):
        with open(md_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        references_map = self._parse_references(lines)
        chunks = []
        current_headers = []
        stop_parsing = False
        for line in lines:
            line = line.strip()
            if "#### **ì°¸ê³  ìë£Œ**" in line:
                stop_parsing = True
            if stop_parsing or not line:
                continue
            header_match = re.match(r"^(#+)\s+(.*)", line)
            if header_match:
                level = len(header_match.group(1))
                header_text = header_match.group(2).strip()
                while len(current_headers) >= level:
                    current_headers.pop()
                current_headers.append(header_text)
                continue
            if line.startswith(("---", ":", "`")):
                continue
            texts_to_process = []
            if line.startswith("|"):
                cells = line.strip("|").split("|")
                texts_to_process.extend(
                    [cell.strip() for cell in cells if cell.strip()]
                )
            else:
                texts_to_process.append(line)
            for text in texts_to_process:
                try:
                    sentences = kss.split_sentences(text)
                except Exception:
                    sentences = [text]
                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue
                    temp_ref_num = None
                    cleaned_sentence = sentence
                    match_end = re.match(r"^(.*?)[\s,.]*([\d,-]+)$", sentence)
                    if match_end:
                        potential_sent = match_end.group(1).strip()
                        potential_ref = match_end.group(2)
                        if potential_sent and self._is_valid_ref_num(potential_ref):
                            cleaned_sentence = potential_sent
                            temp_ref_num = potential_ref
                    if not temp_ref_num:
                        match_start = re.match(r"^([\d,-]+)[\s,.]*(.*)", sentence)
                        if match_start:
                            potential_ref = match_start.group(1)
                            potential_sent = match_start.group(2).strip()
                            if potential_sent and self._is_valid_ref_num(potential_ref):
                                cleaned_sentence = potential_sent
                                temp_ref_num = potential_ref
                    reference_text = None
                    if temp_ref_num:
                        main_ref_num = re.split(r"[, -]", temp_ref_num)[0]
                        if main_ref_num in references_map:
                            reference_text = references_map[main_ref_num]
                    if cleaned_sentence:
                        chunk = {
                            "chunk_id": str(uuid.uuid4()),
                            "file_path": os.path.basename(md_path),
                            "headers": current_headers.copy(),
                            "sentence": cleaned_sentence,
                            "reference_text": reference_text,
                        }
                        chunks.append(chunk)
        return chunks

    def _create_vector_db(self, all_data_items, db_name):
        contents_to_embed = self._prepare_contents(all_data_items)
        embeddings = self._get_embeddings(contents_to_embed)
        faiss_path, json_path = self._build_and_save_faiss_index(
            embeddings, all_data_items, db_name
        )
        return len(embeddings), faiss_path, json_path

    def _prepare_contents(self, data_items):
        contents = []
        for item in data_items:
            sentence = item.get("sentence", "")
            ref_text = item.get("reference_text")

            # ì»¨í…ì¸  ìƒì„±
            if ref_text:
                contents.append(f"ë¬¸ì¥: {sentence}\n\nì¶œì²˜: {ref_text}")
            else:
                contents.append(sentence)

        return contents

    def _get_embeddings(self, contents):
        batch_size = 100
        all_embeddings = []
        total_count = len(contents)

        for i in range(0, total_count, batch_size):
            batch_contents = contents[i : i + batch_size]

            self._update_status(f"ì„ë² ë”© ì¤‘... ({i+len(batch_contents)}/{total_count})")

            result = self.client.models.embed_content(
                model=config.EMBEDDING_MODEL,
                contents=batch_contents,
                config=types.EmbedContentConfig(task_type="RETRIEVAL_DOCUMENT"),
            )
            # ContentEmbedding ê°ì²´ì˜ values ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
            embeddings_values = [embedding.values for embedding in result.embeddings]
            all_embeddings.extend(embeddings_values)

        return np.array(all_embeddings, dtype="float32")

    def _build_and_save_faiss_index(self, embeddings, data_items, db_name):
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)

        faiss_path = f"{db_name}.faiss"
        json_path = f"{db_name}_data.json"

        faiss.write_index(index, faiss_path)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data_items, f, ensure_ascii=False, indent=2)

        return faiss_path, json_path


# LangGraph ìƒíƒœ ì •ì˜
class ReportState(TypedDict):
    topic: str
    outline: str
    report_content: Dict[str, str]
    current_report_text: str
    review_result: dict
    review_history: List[dict]
    review_attempts: int
    formatted_report: str
    final_report_with_refs: str
    progress_message: str


class ReportGeneratorService:
    def __init__(self, progress_queue=None):
        self.progress_queue = progress_queue
        self.index = None
        self.db_data = None
        self.all_vectors = None
        self.client = None
        self.embedding_model = None
        self.chunk_id_map = {}
        self.logger = ConsoleLogger()
        self.analyzer = ReportAnalyzer()
        self.models = {}
        self.thinking_budgets = {}
        self.results_folder = None
        self.logs_folder = None
        self.viz_folder = None
        self.graph = self._build_graph()
        self._configure_api()

    def _configure_api(self):
        load_dotenv()
        if config.GOOGLE_PROJECT_ID:
            # Vertex AI ì‚¬ìš© (ê³µì‹ ë¬¸ì„œ ë°©ì‹)
            self.client = genai.Client(
                vertexai=True, project=config.GOOGLE_PROJECT_ID, location="us-central1"
            )
        else:
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError(
                    "GEMINI_API_KEY ë˜ëŠ” GOOGLE_PROJECT_IDë¥¼ .env íŒŒì¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            # Gemini Developer API ì‚¬ìš© (ê³µì‹ ë¬¸ì„œ ë°©ì‹)
            self.client = genai.Client(api_key=api_key)

    def _update_progress(self, message):
        if self.progress_queue:
            self.progress_queue.put({"progress_message": message})
        else:
            print(message)

    def load_vector_db(self, faiss_path):
        db_name = os.path.basename(faiss_path).replace(".faiss", "")
        data_path = os.path.join(os.path.dirname(faiss_path), f"{db_name}_data.json")

        if not os.path.exists(data_path):
            raise FileNotFoundError(
                f"ë§¤ì¹­ë˜ëŠ” ë°ì´í„° íŒŒì¼(.json)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\nê²½ë¡œ: {data_path}"
            )

        self.index = faiss.read_index(faiss_path)
        with open(data_path, "r", encoding="utf-8") as f:
            self.db_data = json.load(f)

        self.all_vectors = np.array(
            [self.index.reconstruct(i) for i in range(self.index.ntotal)]
        ).astype("float32")
        self.chunk_id_map = {
            item["chunk_id"]: item
            for item in self.db_data
            if "chunk_id" in item and item.get("reference_text")
        }

        total_chunks = len(self.db_data)
        with_references = len(self.chunk_id_map)

        print(f"\n=== Vector DB ë¡œë“œ ì™„ë£Œ: {os.path.basename(faiss_path)} ===")
        print(f"  - ì „ì²´ ì²­í¬: {total_chunks}")
        print(f"  - ì°¸ê³ ë¬¸í—Œ ìˆëŠ” ì²­í¬: {with_references}")
        print(f"  - ì°¸ê³ ë¬¸í—Œ ë¹„ìœ¨: {with_references/total_chunks*100:.1f}%")
        print("=" * 40)

        return self.index.ntotal

    def run_generation_pipeline(self, topic, mode):
        try:
            session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.results_folder = f"results_{mode.lower()}_{session_id}"
            self.logs_folder = os.path.join(self.results_folder, "logs")
            self.viz_folder = os.path.join(self.results_folder, "visualizations")

            os.makedirs(self.results_folder, exist_ok=True)
            os.makedirs(self.logs_folder, exist_ok=True)
            os.makedirs(self.viz_folder, exist_ok=True)

            self.logger.start_logging(session_id)
            self.logger.add_log(
                "SYSTEM",
                f"========== {'Extreme' if mode == 'Bypass' else 'Standard'} Report Generation Pipeline Start ==========",
            )

            if mode == "Bypass":
                self.models = {"bypass_generation": config.BYPASS_MODEL}
                graph = self._build_bypass_graph()
                initial_state = ReportState(
                    topic=topic,
                    outline="",
                    report_content={},
                    current_report_text="",
                    review_result={},
                    review_history=[],
                    review_attempts=0,
                    formatted_report="",
                    final_report_with_refs="",
                    progress_message="0/2: ìµìŠ¤íŠ¸ë¦¼ íŒŒì´í”„ë¼ì¸ ì‹œì‘...",
                )
            else:
                if mode == "Production":
                    self.models = config.PRODUCTION_MODELS
                    self.thinking_budgets = config.PRODUCTION_THINKING_BUDGETS
                else:
                    self.models = config.TEST_MODELS
                    self.thinking_budgets = config.TEST_THINKING_BUDGETS

                graph = self.graph
                initial_state = ReportState(
                    topic=topic,
                    outline="",
                    report_content={},
                    current_report_text="",
                    review_result={},
                    review_history=[],
                    review_attempts=0,
                    formatted_report="",
                    final_report_with_refs="",
                    progress_message="0/6: íŒŒì´í”„ë¼ì¸ ì‹œì‘...",
                )

            final_state = None
            for event in graph.stream(initial_state, {"recursion_limit": 15}):
                node_name = list(event.keys())[0]
                node_output = event[node_name]

                for key, value in node_output.items():
                    if isinstance(value, str) and len(value) > 300:
                        self.logger.add_log("DEBUG", f"  - {key}: (ë‚´ìš©ì´ ê¸¸ì–´ ìƒëµë¨)")
                    elif key not in ["client", "root"]:
                        self.logger.add_log("DEBUG", f"  - {key}: {value}")

                if (
                    "progress_message" in node_output
                    and node_output["progress_message"]
                ):
                    self._update_progress(node_output["progress_message"])

                final_state = node_output

            self.logger.add_log(
                "SYSTEM",
                f"========== {'Extreme' if mode == 'Bypass' else 'Standard'} Report Generation Pipeline End ==========",
            )

            if not final_state:
                raise Exception("ê·¸ë˜í”„ ì‹¤í–‰ì´ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            final_report_with_refs = final_state.get(
                "final_report_with_refs", "ì˜¤ë¥˜: ìµœì¢… ë³´ê³ ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            )

            now = datetime.now()
            date_str = now.strftime("%Y%m%d_%H%M%S")
            report_filename = os.path.join(
                self.results_folder, f"final_report_{date_str}.md"
            )
            self.logger.add_log(
                "INFO",
                f"[{'2/2' if mode == 'Bypass' else '6/6'}] ë³´ê³ ì„œ íŒŒì¼ ì €ì¥ ì¤‘... -> '{report_filename}'",
            )

            with open(report_filename, "w", encoding="utf-8") as f:
                f.write(final_report_with_refs)

            if mode != "Bypass":
                full_log_path, node_log_paths = self.logger.save_logs(self.logs_folder)

                dashboard_path = self.analyzer.create_visualization_dashboard(
                    self.logger, final_state, self.viz_folder
                )
            else:
                full_log_path = self.logger.save_logs(self.logs_folder, is_bypass=True)
                dashboard_path = None

            self.logger.add_log(
                "SUCCESS",
                f"ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ í´ë”: {self.results_folder}",
            )

            if self.progress_queue:
                self.progress_queue.put(
                    {
                        "final_report": final_report_with_refs,
                        "dashboard_path": dashboard_path,
                        "log_path": full_log_path,
                    }
                )

        except Exception as e:
            import traceback

            traceback.print_exc()
            self.logger.add_log("ERROR", f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if self.progress_queue:
                self.progress_queue.put({"error": f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"})
        finally:
            if self.progress_queue:
                self.progress_queue.put({"generation_done": True})

    def _get_model_for_task(self, task_name):
        return self.models.get(task_name, "gemini-1.5-flash-latest")

    def _get_generation_config(self, task_name, tools=None):
        budget = self.thinking_budgets.get(task_name)
        thinking_config = (
            types.ThinkingConfig(thinking_budget=budget) if budget is not None else None
        )
        return types.GenerateContentConfig(thinking_config=thinking_config, tools=tools)

    def _search_similar_documents(self, query, k=10):
        query_embedding = (
            self.client.models.embed_content(
                model=config.EMBEDDING_MODEL,
                contents=[query],
                config=types.EmbedContentConfig(task_type="RETRIEVAL_QUERY"),
            )
            .embeddings[0]
            .values
        )  # ContentEmbeddingì˜ values ì†ì„±ìœ¼ë¡œ ì ‘ê·¼
        candidate_k = min(k * 3, len(self.db_data))
        distances, indices = self.index.search(
            np.array([query_embedding], dtype="float32"), candidate_k
        )
        candidates = [self.db_data[i] for i in indices[0]]
        with_refs = [c for c in candidates if c.get("reference_text")]
        without_refs = [c for c in candidates if not c.get("reference_text")]
        result = with_refs[:k] + without_refs[: max(0, k - len(with_refs))]
        return result[:k]

    def _extract_key_themes(self):
        num_clusters = min(config.NUM_CLUSTERS_FOR_OUTLINE, self.index.ntotal)
        kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)
        kmeans.fit(self.all_vectors)
        representative_indices = []
        for i in range(num_clusters):
            cluster_indices = np.where(kmeans.labels_ == i)[0]
            if len(cluster_indices) > 0:
                cluster_center = kmeans.cluster_centers_[i]
                distances = faiss.pairwise_distances(
                    cluster_center.reshape(1, -1), self.all_vectors[cluster_indices]
                )
                closest_in_cluster = np.argmin(distances)
                representative_indices.append(cluster_indices[closest_in_cluster])
        return [self.db_data[i] for i in representative_indices]

    def _create_search_queries_for_section(self, header, topic, outline):
        """LLMì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì„¹ì…˜ì— ëŒ€í•œ ë‹¤ì–‘í•˜ê³  êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        self.logger.add_log("DEBUG", f"'{header}' ì„¹ì…˜ì— ëŒ€í•œ ê²€ìƒ‰ì–´ ìƒì„± ì¤‘...")

        prompt = f"""
        ë‹¹ì‹ ì€ íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì •ë³´ ìˆ˜ì§‘ì„ ìœ„í•´, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•  ê²€ìƒ‰ì–´ë¥¼ ë§Œë“œëŠ” ì „ë¬¸ ì—°êµ¬ì›ì…ë‹ˆë‹¤.
        ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, '{header}' ì„¹ì…˜ì„ ì‘ì„±í•˜ëŠ” ë° í•„ìš”í•œ ê°€ì¥ íš¨ê³¼ì ì¸ ê²€ìƒ‰ì–´ {config.NUM_SEARCH_QUERIES}ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

        --- ì „ì²´ ë³´ê³ ì„œ ì£¼ì œ ---
        {topic}

        --- ì „ì²´ ë³´ê³ ì„œ ê°œìš” ---
        {outline}
        --- ë ---

        **ì§€ì‹œì‚¬í•­:**
        1. **ë‹¤ì–‘ì„±:** ìƒì„±ë˜ëŠ” ê²€ìƒ‰ì–´ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ê´€ì ê³¼ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ë™ì¼í•œ ê²€ìƒ‰ì–´ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
        2. **êµ¬ì²´ì„±:** "ACPì˜ ì—­ì‚¬"ì™€ ê°™ì€ í¬ê´„ì ì¸ ê²€ìƒ‰ì–´ ëŒ€ì‹ , "ë²¨ê¸°ì— ACP ì œë„ì˜ ì´ˆê¸° íŒë¡€" ë˜ëŠ” "2022ë…„ ACP ë²•ì œí™”ì˜ ì£¼ìš” ë™ì¸"ê³¼ ê°™ì´ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“œì„¸ìš”.
        3. **ì¶œë ¥ í˜•ì‹:** ê° ê²€ìƒ‰ì–´ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬, ì˜¤ì§ ê²€ìƒ‰ì–´ ëª©ë¡ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        """
        model = self._get_model_for_task("query_generation")
        response = self.client.models.generate_content(model=model, contents=prompt)
        queries = [q.strip() for q in response.text.split("\n") if q.strip()]

        self.logger.add_log("DEBUG", f"ìƒì„±ëœ ê²€ìƒ‰ì–´: {queries}")
        return queries

    def _generate_outline_logic(self, topic):
        key_theme_contexts = self._extract_key_themes()
        topic_specific_contexts = self._search_similar_documents(
            topic, k=config.K_FOR_TOPIC_SEARCH
        )
        combined_contexts = {
            item["sentence"]: item
            for item in key_theme_contexts + topic_specific_contexts
        }.values()
        context_str = "\n\n---\n\n".join(
            [
                f"ë¬¸ì„œ: {c['file_path']}\nëª©ì°¨: {' > '.join(c['headers'])}\në¬¸ì¥: {c['sentence']}"
                for c in combined_contexts
            ]
        )
        prompt = config.OUTLINE_PROMPT_TEMPLATE.format(
            report_purpose=config.REPORT_PURPOSE,
            topic=topic,
            guideline=config.EDITORIAL_GUIDELINE,
            context_str=context_str,
        )
        model = self._get_model_for_task("outline_generation")
        generation_config = self._get_generation_config("outline_generation")
        response = self.client.models.generate_content(
            model=model, contents=prompt, config=generation_config
        )
        return response.text

    def _generate_single_section(
        self, header, topic, outline, improvement_instructions=""
    ):
        # ì´ í•¨ìˆ˜ëŠ” `node_generate_draft`ì™€ `node_regenerate_sections`ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
        self.logger.add_log("DEBUG", f"'{header}' ì„¹ì…˜ ìƒì„± ì‹œì‘...")

        # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•˜ê³  êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ ìƒì„±
        search_queries = self._create_search_queries_for_section(header, topic, outline)

        # 2. ìƒì„±ëœ ê° ê²€ìƒ‰ì–´ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©
        all_contexts = {}
        for query in search_queries:
            contexts = self._search_similar_documents(
                query, k=config.K_FOR_SECTION_DRAFT
            )
            for c in contexts:
                all_contexts[c["chunk_id"]] = c  # chunk_idë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°

        final_contexts = list(all_contexts.values())
        self.logger.add_log(
            "INFO",
            f"'{header}' ì„¹ì…˜ ìœ„í•´ ì´ {len(final_contexts)}ê°œì˜ ê³ ìœ í•œ ì°¸ê³ ìë£Œ ê²€ìƒ‰ ì™„ë£Œ.",
        )

        context_str = "\n\n---\n\n".join(
            [
                f"chunk_id: {c['chunk_id']}\në¬¸ì„œ: {c['file_path']}\nëª©ì°¨: {' > '.join(c['headers'])}\në¬¸ì¥: {c['sentence']}\nì°¸ê³ ë¬¸í—Œ: {c.get('reference_text', 'N/A')}"
                for c in final_contexts
            ]
        )

        improvement_prompt = ""
        if improvement_instructions:
            improvement_prompt = f"""
            --- ê°œì„  ì§€ì‹œì‚¬í•­ ---
            ì´ì „ ë²„ì „ì— ëŒ€í•œ í¸ì§‘íŒ€ì˜ ê²€í†  ê²°ê³¼, ë‹¤ìŒê³¼ ê°™ì€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ì§€ì‹œì‚¬í•­ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ë‚´ìš©ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            - {improvement_instructions}
            --- ë ---
            """

        prompt = config.DRAFT_SECTION_PROMPT_TEMPLATE.format(
            report_purpose=config.REPORT_PURPOSE,
            topic=topic,
            improvement_prompt=improvement_prompt,
            guideline=config.EDITORIAL_GUIDELINE,
            context_str=context_str,
            header=header,
        )
        model = self._get_model_for_task("draft_generation")
        generation_config = self._get_generation_config("draft_generation")
        response = self.client.models.generate_content(
            model=model, contents=prompt, config=generation_config
        )
        self.logger.add_log(
            "DEBUG", f"'{header}' ì„¹ì…˜ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(response.text)}ì)"
        )
        return response.text

    def _editorial_review_logic(self, report_text, outline):
        self.logger.add_log("INFO", "í¸ì§‘ì¥ ê²€í†  ë¡œì§ ì‹œì‘...")
        prompt = config.EDITORIAL_REVIEW_PROMPT_TEMPLATE.format(
            report_purpose=config.REPORT_PURPOSE,
            outline=outline,
            guideline=config.EDITORIAL_GUIDELINE,
            report_text=report_text,
        )
        model = self._get_model_for_task("editorial_review")
        generation_config = self._get_generation_config("editorial_review")
        response = self.client.models.generate_content(
            model=model, contents=prompt, config=generation_config
        )

        try:
            json_text = re.search(
                r"```json\n(.*?)\n```", response.text, re.DOTALL
            ).group(1)
            result = json.loads(json_text)
            self.logger.add_log(
                "SUCCESS",
                "í¸ì§‘ì¥ ê²€í†  ì™„ë£Œ. ê°œì„  í•„ìš” ì„¹ì…˜: "
                + str(len(result.get("sections_to_improve", []))),
            )
            return result
        except (AttributeError, json.JSONDecodeError) as e:
            self.logger.add_log("ERROR", f"í¸ì§‘ì¥ ê²€í†  ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "overall_comment": f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨. ëª¨ë¸ì´ ìœ íš¨í•œ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\nì˜¤ë¥˜: {e}",
                "review_passed": False,
                "sections_to_improve": [],
                "raw_response": response.text,  # ì›ë³¸ ì‘ë‹µ ì¶”ê°€
            }

    def _final_formatting_logic(self, report_text):
        self.logger.add_log("INFO", "ìµœì¢… ì„œì‹ ì •ë¦¬ ë¡œì§ ì‹œì‘...")
        prompt = config.FINAL_FORMATTING_PROMPT_TEMPLATE.format(
            report_text=report_text, guideline=config.EDITORIAL_GUIDELINE
        )
        model = self._get_model_for_task("final_formatting")
        generation_config = self._get_generation_config("final_formatting")
        response = self.client.models.generate_content(
            model=model, contents=prompt, config=generation_config
        )
        self.logger.add_log("SUCCESS", "ìµœì¢… ì„œì‹ ì •ë¦¬ ì™„ë£Œ.")
        return response.text

    def _finalize_citations_logic(self, final_text):
        self.logger.add_log("INFO", "ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ë¡œì§ ì‹œì‘...")

        # 1. ë³¸ë¬¸ì—ì„œ ëª¨ë“  ê°ì£¼ IDë¥¼ *ìˆœì„œëŒ€ë¡œ* ì°¾ìŠµë‹ˆë‹¤. (set ì‚¬ìš© ì•ˆ í•¨, ^ëŠ” ì„ íƒì‚¬í•­)
        ordered_footnote_ids = re.findall(r"\[\^?([\w-]+)\]", final_text)

        if not ordered_footnote_ids:
            self.logger.add_log("INFO", "ë³¸ë¬¸ì— ì²˜ë¦¬í•  ìœ íš¨í•œ ê°ì£¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return re.sub(r"\[\^?([\w-]+)\]", "", final_text)

        # 2. ê°ì£¼ê°€ ì²˜ìŒ ë“±ì¥í•˜ëŠ” ìˆœì„œëŒ€ë¡œ ê³ ìœ í•œ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤.
        unique_references_in_order = []
        seen_refs = set()
        for chunk_id in ordered_footnote_ids:
            chunk_data = self.chunk_id_map.get(chunk_id)
            if chunk_data:
                ref_text = chunk_data.get("reference_text")
                if ref_text and ref_text not in seen_refs:
                    unique_references_in_order.append(ref_text)
                    seen_refs.add(ref_text)

        if not unique_references_in_order:
            self.logger.add_log(
                "INFO", "ë³¸ë¬¸ì— ìœ íš¨í•œ ì°¸ê³ ë¬¸í—Œì´ ì—°ê²°ëœ ê°ì£¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            )
            return re.sub(r"\[\^?([\w-]+)\]", "", final_text)

        # 3. ë“±ì¥ ìˆœì„œì— ë”°ë¼ ì°¸ê³ ë¬¸í—Œì— ë²ˆí˜¸ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
        ref_to_num_map = {
            ref: i + 1 for i, ref in enumerate(unique_references_in_order)
        }

        # 4. ë³¸ë¬¸ì˜ ê°ì£¼ IDë¥¼ ìˆœì„œì— ë§ëŠ” ë²ˆí˜¸ë¡œ êµì²´í•©ë‹ˆë‹¤.
        def replace_footnote(match):
            chunk_id = match.group(1)
            chunk_data = self.chunk_id_map.get(chunk_id)
            if chunk_data:
                ref_text = chunk_data.get("reference_text")
                if ref_text and ref_text in ref_to_num_map:
                    return f"[{ref_to_num_map[ref_text]}]"
            return ""  # ìœ íš¨í•˜ì§€ ì•Šì€ ê°ì£¼ëŠ” ì œê±°

        final_text_with_numbered_refs = re.sub(
            r"\[\^?([\w-]+)\]", replace_footnote, final_text
        )

        # 5. ë³´ê³ ì„œ ëì— ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        references_section = "\n\n---\n\n## ì°¸ê³ ë¬¸í—Œ\n\n"
        for i, ref in enumerate(unique_references_in_order):
            references_section += f"{i+1}. {ref}\n"

        self.logger.add_log(
            "SUCCESS",
            f"{len(unique_references_in_order)}ê°œì˜ ê³ ìœ  ì°¸ê³ ë¬¸í—Œì„ ìˆœì„œì— ë§ê²Œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.",
        )
        return final_text_with_numbered_refs + references_section

    def node_generate_bypassed_report(self, state: ReportState):
        """
        Extreme ëª¨ë“œ: ê°œìš”ë¥¼ ë°”íƒ•ìœ¼ë¡œ Search Groundingê³¼ ì°¸ê³  ìë£Œë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ í•œë²ˆì— ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        self.logger.set_current_node("generate_bypassed_report")
        self.logger.add_log("INFO", "[1/2] ìµìŠ¤íŠ¸ë¦¼ ëª¨ë“œë¡œ ì „ì²´ ë³´ê³ ì„œ ìƒì„± ì‹œì‘")
        self._update_progress("[1/2] ìµìŠ¤íŠ¸ë¦¼ ëª¨ë“œë¡œ ì „ì²´ ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")

        topic = state["topic"]
        outline = state["outline"]

        # 1. DBì—ì„œ ì£¼ì œ ê´€ë ¨ ë¬¸ì„œë“¤ì„ ê´‘ë²”ìœ„í•˜ê²Œ ìˆ˜ì§‘
        self.logger.add_log("INFO", "DBì—ì„œ ì£¼ì œ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘...")

        # ì£¼ì œ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ê¸°ë³¸ ë¬¸ì„œë“¤ ìˆ˜ì§‘
        topic_documents = self._search_similar_documents(topic, k=50)

        # ê°œìš”ì˜ ê° ì„¹ì…˜ë³„ë¡œë„ ì¶”ê°€ ê²€ìƒ‰ ìˆ˜í–‰
        outline_sections = re.findall(r"^#+\s+(.+)", outline, re.MULTILINE)
        section_documents = []
        for section in outline_sections[:10]:  # ìƒìœ„ 10ê°œ ì„¹ì…˜ë§Œ
            section_docs = self._search_similar_documents(section, k=20)
            section_documents.extend(section_docs)

        # ì¤‘ë³µ ì œê±° ë° í†µí•©
        all_documents = {}
        for doc in topic_documents + section_documents:
            doc_key = doc.get(
                "chunk_id", f"{doc.get('file_path', '')}_{doc.get('sentence', '')[:50]}"
            )
            all_documents[doc_key] = doc

        final_documents = list(all_documents.values())

        # ì°¸ê³ ë¬¸í—Œì´ ìˆëŠ” ë¬¸ì„œ ìš°ì„  ì •ë ¬
        final_documents.sort(
            key=lambda x: (
                1 if x.get("reference_text") else 0,  # ì°¸ê³ ë¬¸í—Œ ìˆëŠ” ê²ƒ ìš°ì„ 
                -len(x.get("sentence", "")),  # ê¸´ ë¬¸ì¥ ìš°ì„ 
            ),
            reverse=True,
        )

        self.logger.add_log(
            "INFO", f"ì´ {len(final_documents)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤."
        )

        # 2. ë¬¸ì„œ ë‚´ìš©ì„ êµ¬ì¡°ì ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        data_context = self._prepare_extensive_db_context(
            final_documents[:100]
        )  # ìƒìœ„ 100ê°œ ë¬¸ì„œ ì‚¬ìš©

        # System Instructionìœ¼ë¡œ ëª¨ë¸ì˜ ê¸°ë³¸ ë™ì‘ ì •ì˜
        system_instruction = """
        ë‹¹ì‹ ì€ ë²•ë¥  ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ë¡œì„œ, ë§¤ìš° ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ì—°êµ¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ ì—°êµ¬ì›ì…ë‹ˆë‹¤. 
        
        í•µì‹¬ ì›ì¹™:
        1. ì œê³µëœ ì°¸ê³  ìë£Œë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‚´ìš©ì„ ì‘ì„±í•˜ì„¸ìš”
        2. ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ì •ë³´ë¥¼ ë³´ì™„í•˜ì—¬ ì¢…í•©ì ì¸ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”
        3. ê° ì£¼ì¥ë§ˆë‹¤ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì  ê·¼ê±°ì™€ ì¶œì²˜ë¥¼ ì œì‹œí•˜ì„¸ìš”
        4. ì ˆëŒ€ë¡œ ê°„ëµí•˜ê²Œ ì‘ì„±í•˜ì§€ ë§ê³ , ê°€ëŠ¥í•œ í•œ ê°€ì¥ ìƒì„¸í•˜ê³  ê¸´ ë‚´ìš©ì„ ì œê³µí•˜ì„¸ìš”
        5. ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í™œìš©í•˜ì—¬ ì™„ì „í•œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”
        """

        prompt = f"""
        ë‹¤ìŒ ì£¼ì œì™€ ê°œìš”ì— ë”°ë¼ **ê·¹ë„ë¡œ ìƒì„¸í•˜ê³  í¬ê´„ì ì¸** ë²•ë¥  ì—°êµ¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ## ë³´ê³ ì„œ ì£¼ì œ
        {topic}

        ## ë³´ê³ ì„œ ê°œìš”  
        {outline}

        ## ğŸ“š **ì œê³µëœ ì°¸ê³  ìë£Œ** 
        
        **ì¤‘ìš”: ì•„ë˜ ìë£Œë“¤ì€ ë§¤ìš° ê·€ì¤‘í•œ 1ì°¨ ìë£Œì…ë‹ˆë‹¤. ì´ ë‚´ìš©ë“¤ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.**
        
        {data_context}
        
        ---

        ## ğŸ¯ **ê·¹í•œ ìƒì„¸ë„ ìš”êµ¬ì‚¬í•­**

        **ëª©í‘œ ê¸¸ì´: ìµœì†Œ 30,000ì ì´ìƒ (ì•½ 50-60í˜ì´ì§€ ë¶„ëŸ‰)**

        ### ğŸ“‹ **ì‘ì„± ë°©ë²•ë¡ **
        
        **A. ì°¸ê³  ìë£Œ í™œìš© ìš°ì„  ì›ì¹™**:
        - ìœ„ì— ì œê³µëœ ì°¸ê³  ìë£Œë¥¼ ë°˜ë“œì‹œ ìµœìš°ì„ ìœ¼ë¡œ í™œìš©
        - ê° ì„¹ì…˜ë§ˆë‹¤ ê´€ë ¨ëœ ë‚´ìš©ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ê³  ë¶„ì„
        - êµ¬ì²´ì ì¸ ì‚¬ë¡€, íŒë¡€, ë²•ì¡°ë¬¸ì„ ì ê·¹ í™œìš©
        - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ê°€ ìˆëŠ” ìë£ŒëŠ” ê·¸ ì¶œì²˜ê¹Œì§€ ëª…ì‹œí•˜ì—¬ í™œìš©

        **B. ì›¹ ê²€ìƒ‰ ë³´ì™„ í™œìš©**:
        - ì œê³µëœ ìë£Œë¡œ ê¸°ë³¸ í‹€ì„ êµ¬ì„±í•œ í›„, ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ì •ë³´ ë³´ì™„
        - ìµœê·¼ ë™í–¥ì´ë‚˜ í•´ì™¸ ì‚¬ë¡€ëŠ” ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë³´ê°•
        - ëª¨ë“  ìë£Œì™€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ì¢…í•©ì  ë¶„ì„

        ### ğŸ“‹ **ì„¹ì…˜ë³„ ì„¸ë¶€ ìš”êµ¬ì‚¬í•­**
        1. **ì„œë¡  (ìµœì†Œ 4,000ì)**:
           - ì œê³µëœ ë°°ê²½ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œ ìƒí™© ë¶„ì„
           - ì œë„ì˜ ì—­ì‚¬ì  ë°œì „ ê³¼ì •ì„ ìƒì„¸ ì„¤ëª…
           - ì—°êµ¬ì˜ í•„ìš”ì„±ì„ êµ¬ì²´ì  ì‚¬ë¡€ë¡œ ì…ì¦
           - ë°©ë²•ë¡ ê³¼ ì ‘ê·¼ ë°©ì‹ì„ ëª…í™•íˆ ê¸°ìˆ 

        2. **ê° ë³¸ë¡  ì„¹ì…˜ (ê°ê° ìµœì†Œ 5,000-6,000ì)**:
           - ë²•ì¡°ë¬¸, íŒë¡€, ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…
           - ê°êµ­ ë¹„êµ ìë£Œë¥¼ ì›ë¬¸ì„ ì¸ìš©í•˜ì—¬ ìƒì„¸ ë¶„ì„
           - ì „ë¬¸ê°€ ì˜ê²¬ì´ë‚˜ ì—°êµ¬ ê²°ê³¼ë¥¼ ì ê·¹ í™œìš©
           - ì‹¤ë¬´ ì‚¬ë¡€ì™€ ì ˆì°¨ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
           - ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ë™í–¥ê³¼ ë³€í™” ì‚¬í•­ ë³´ì™„

        3. **ê²°ë¡  (ìµœì†Œ 3,000ì)**:
           - ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ í•µì‹¬ ë°œê²¬ì‚¬í•­ ì •ë¦¬
           - ì •ì±…ì  í•¨ì˜ë¥¼ êµ¬ì²´ì  ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œ
           - í–¥í›„ ì—°êµ¬ ê³¼ì œì™€ ê°œì„  ë°©ì•ˆì„ ìƒì„¸íˆ ë…¼ì˜

        ### ğŸ” **ìë£Œ í™œìš© ê·¹ëŒ€í™” ì§€ì¹¨**

        **1. ì¸ìš© ë° ì¶œì²˜ í‘œê¸°**:
        - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìë£Œë¥¼ ì¸ìš©í•  ë•ŒëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì¶œì²˜ë¥¼ ëª…ì‹œ
        - ì°¸ê³ ë¬¸í—Œì´ ìˆëŠ” ìë£ŒëŠ” ê·¸ ì›ì¶œì²˜ë„ í•¨ê»˜ í‘œê¸°
        - ê° ë¬¸ë‹¨ë§ˆë‹¤ ê´€ë ¨ ìë£Œë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©

        **2. êµ¬ì²´ì„±ê³¼ ì •í™•ì„±**:
        - ì¶”ìƒì  ì„¤ëª… ëŒ€ì‹  êµ¬ì²´ì  ì‚¬ë¡€ì™€ ìˆ˜ì¹˜ ë°ì´í„° í™œìš©
        - ë²•ì¡°ë¬¸ ë²ˆí˜¸, íŒë¡€ ë²ˆí˜¸ ë“±ì„ ì •í™•íˆ ì¸ìš©
        - ì „ë¬¸ ìš©ì–´ë‚˜ ê°œë…ì€ ì •í™•í•œ ì •ì˜ì™€ ì„¤ëª…ì„ í™œìš©

        **3. ì™„ì „ì„± ì¶”êµ¬**:
        - ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ ê²€í† í•˜ê³  í™œìš©
        - ì—¬ëŸ¬ êµ­ê°€ì˜ ìë£Œê°€ ìˆë‹¤ë©´ ëª¨ë‘ ë¹„êµ ë¶„ì„ì— í¬í•¨
        - ì‹œê³„ì—´ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë³€í™” ì¶”ì´ê¹Œì§€ ë¶„ì„

        ### âœ… **í’ˆì§ˆ ë³´ì¦ ìš”êµ¬ì‚¬í•­**
        1. ì œê³µëœ ìë£Œ í™œìš©ë¥  ìµœì†Œ 70% ì´ìƒ (ì „ì²´ ë‚´ìš© ì¤‘ ìë£Œ ê¸°ë°˜ ë‚´ìš© ë¹„ìœ¨)
        2. ê° ë¬¸ë‹¨ë§ˆë‹¤ ìµœì†Œ 300-500ì ì´ìƒì˜ ì‹¤ì§ˆì  ë‚´ìš© í¬í•¨
        3. ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìë£Œ ë˜ëŠ” ì›¹ ê²€ìƒ‰ ê·¼ê±° ì œì‹œ
        4. êµ¬ì²´ì  ì‚¬ë¡€, ìˆ˜ì¹˜, ì¸ìš©ë¬¸ì„ í’ë¶€í•˜ê²Œ í™œìš©
        5. ë…¼ë¦¬ì  ì—°ê²°ê³ ë¦¬ì™€ ì²´ê³„ì  êµ¬ì„±ìœ¼ë¡œ ê°€ë…ì„± í™•ë³´

        **ğŸ”¥ ìµœì¢… ëª©í‘œ: ë°©ëŒ€í•œ ìë£Œë¥¼ ì™„ì „íˆ í™œìš©í•˜ì—¬, í•´ë‹¹ ë¶„ì•¼ì˜ ê²°ì •ì  ì°¸ê³ ë¬¸í—Œì´ ë  ìˆ˜ ìˆì„ ë§Œí¼ ì™„ë²½í•˜ê³  í¬ê´„ì ì¸ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì„¸ìš”.**
        """

        model_name = self.models.get("bypass_generation", "gemini-2.5-pro")

        # Search Grounding ë° ê·¹í•œ ì„±ëŠ¥ ì„¤ì •
        tools = [types.Tool(google_search=types.GoogleSearch())]

        config = types.GenerateContentConfig(
            # ê¸°ë³¸ ìƒì„± ì„¤ì •
            system_instruction=system_instruction,
            tools=tools,
            # í† í° ë° ê¸¸ì´ ì„¤ì • (ê·¹í•œ ìµœì í™”)
            max_output_tokens=65536,  # ìµœëŒ€ ê°€ëŠ¥ í† í° ìˆ˜ (ì•½ 50,000ì)
            # ì°½ì˜ì„± ë° ë‹¤ì–‘ì„± ì¡°ì ˆ
            temperature=0.8,  # ë” ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ í‘œí˜„
            top_p=0.95,  # ë‹¤ì–‘í•œ ë‹¨ì–´ ì„ íƒ í—ˆìš©
            top_k=40,  # ì ì ˆí•œ ì–´íœ˜ ë‹¤ì–‘ì„±
            # ë‹¤ì¤‘ í›„ë³´ ìƒì„±ìœ¼ë¡œ ìµœê³  í’ˆì§ˆ í™•ë³´
            candidate_count=1,  # ì•ˆì •ì„±ì„ ìœ„í•´ 1ê°œë¡œ ì„¤ì •
            # ê¹Šì´ ìˆëŠ” ì‚¬ê³ ë¥¼ ìœ„í•œ ì„¤ì •
            thinking_config=types.ThinkingConfig(
                thinking_budget=32768,  # ìµœëŒ€ ì‚¬ê³  ì˜ˆì‚°
                include_thoughts=False,  # ì‚¬ê³  ê³¼ì •ì€ í¬í•¨í•˜ì§€ ì•ŠìŒ
            ),
            # ì•ˆì „ ë° ì‘ë‹µ í˜•ì‹ ì„¤ì •
            response_mime_type="text/plain",
            # ì •ì§€ ì¡°ê±´ (ë” ê¸´ ìƒì„±ì„ ìœ„í•´ ì œê±°)
            stop_sequences=[],
        )

        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ í™•ì¸ ë° ë¡œê¹…
        prompt_length = len(prompt)
        estimated_tokens = prompt_length // 4  # ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚°

        self.logger.add_log(
            "INFO", f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {prompt_length:,}ì (ì•½ {estimated_tokens:,} í† í°)"
        )
        self.logger.add_log(
            "INFO",
            f"ì°¸ê³ ë¬¸ì„œ {len(final_documents)}ê°œë¥¼ í¬í•¨í•˜ì—¬ Gemini 2.5 Pro ê·¹í•œ ì„¤ì •ìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„± ì‹œì‘",
        )

        response = self.client.models.generate_content(
            model=model_name, contents=prompt, config=config
        )

        # ì°¸ê³ ë¬¸í—Œ ì¶”ì¶œ ë° í¬ë§¤íŒ…
        try:
            grounding_metadata = getattr(
                response.candidates[0], "grounding_metadata", None
            )
            if grounding_metadata:
                web_search_queries = grounding_metadata.web_search_queries or []
                grounding_chunks = grounding_metadata.grounding_chunks or []
            else:
                web_search_queries = []
                grounding_chunks = []

            self.logger.add_log(
                "INFO", f"Grounding ê²€ìƒ‰ì–´ ìˆ˜: {len(web_search_queries)}"
            )
            self.logger.add_log("DEBUG", f"ê²€ìƒ‰ì–´ ëª©ë¡: {web_search_queries}")

            references_section = "\n\n---\n\n## ğŸ“š ì°¸ê³ ë¬¸í—Œ\n\n"

            # DB ì°¸ê³ ë¬¸í—Œ ì¶”ê°€
            db_references = set()
            for doc in final_documents:
                if doc.get("reference_text"):
                    db_references.add(doc["reference_text"])

            if db_references:
                references_section += "### ğŸ“– DB ì›ë¬¸ ì°¸ê³ ìë£Œ\n\n"
                for i, ref in enumerate(sorted(db_references), 1):
                    references_section += f"{i}. {ref}\n\n"

            # ì›¹ ê²€ìƒ‰ ì°¸ê³ ë¬¸í—Œ ì¶”ê°€ (ê°œì„ ëœ URL ì¶”ì¶œ)
            unique_refs = {}
            for chunk in grounding_chunks:
                if chunk.web:
                    # ì›ë³¸ URL ì¶”ì¶œ ì‹œë„
                    uri = chunk.web.uri
                    title = chunk.web.title or "ì œëª© ì—†ìŒ"

                    # ë¦¬ë””ë ‰ì…˜ URLì¸ ê²½ìš° ì›ë³¸ URL ì¶”ì¶œ ì‹œë„
                    if "grounding-api-redirect" in uri:
                        # grounding chunkì—ì„œ ì¶”ê°€ ì •ë³´ í™•ì¸
                        if (
                            hasattr(chunk, "retrieved_context")
                            and chunk.retrieved_context
                        ):
                            if hasattr(chunk.retrieved_context, "uri"):
                                original_uri = chunk.retrieved_context.uri
                                if (
                                    original_uri
                                    and not "grounding-api-redirect" in original_uri
                                ):
                                    uri = original_uri

                        # titleì—ì„œ ë„ë©”ì¸ ì •ë³´ ì¶”ì¶œí•˜ì—¬ í‘œì‹œ
                        if title != "ì œëª© ì—†ìŒ":
                            display_text = f"{title} (ê²€ìƒ‰ ê²°ê³¼)"
                        else:
                            display_text = "ì›¹ ê²€ìƒ‰ ê²°ê³¼"
                    else:
                        display_text = title

                    if uri not in unique_refs:
                        unique_refs[uri] = display_text

            if unique_refs:
                start_num = len(db_references) + 1
                references_section += "### ğŸŒ ì›¹ ê¸°ë°˜ ì°¸ê³ ìë£Œ\n\n"
                for i, (uri, display_text) in enumerate(unique_refs.items(), start_num):
                    if "grounding-api-redirect" in uri:
                        # ë¦¬ë””ë ‰ì…˜ ë§í¬ì¸ ê²½ìš° ë§í¬ì™€ í•¨ê»˜ ì„¤ëª… ì¶”ê°€
                        references_section += f"{i}. **{display_text}**  \n   {uri}  \n   *(Google Search Groundingì„ í†µí•´ ìˆ˜ì§‘ëœ ìë£Œ)*\n\n"
                    else:
                        # ì§ì ‘ ë§í¬ì¸ ê²½ìš° URL í‘œì‹œ
                        references_section += f"{i}. **{display_text}**  \n   {uri}\n\n"

            final_report_text = response.text + references_section

            total_refs = len(db_references) + len(unique_refs)
            self.logger.add_log(
                "SUCCESS",
                f"ì´ {total_refs}ê°œ ì°¸ê³ ë¬¸í—Œ ì¶”ê°€ (DB: {len(db_references)}ê°œ, ì›¹: {len(unique_refs)}ê°œ)",
            )

        except (AttributeError, IndexError) as e:
            self.logger.add_log(
                "WARN",
                f"ì°¸ê³ ë¬¸í—Œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}. ëª¨ë¸ ì‘ë‹µë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.",
            )
            final_report_text = response.text

        # ìƒì„± ê²°ê³¼ í†µê³„
        char_count = len(final_report_text)
        word_count = len(final_report_text.split())
        data_utilization = (len(final_documents) / max(len(self.db_data), 1)) * 100

        self.logger.add_log(
            "SUCCESS",
            f"ğŸ‰ ìë£Œ ì™„ì „ í™œìš© ìµìŠ¤íŠ¸ë¦¼ ëª¨ë“œ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!\n"
            f"   ğŸ“Š í†µê³„: {char_count:,}ì ({word_count:,}ë‹¨ì–´)\n"
            f"   ğŸ¯ ëª©í‘œ ë‹¬ì„±ë¥ : {(char_count/30000)*100:.1f}% (ëª©í‘œ: 30,000ì)\n"
            f"   ğŸ’¾ ìë£Œ í™œìš©ë¥ : {data_utilization:.1f}% ({len(final_documents)}/{len(self.db_data)}ê°œ ë¬¸ì„œ)",
        )

        return {
            "final_report_with_refs": final_report_text,
            "progress_message": "1/2: ìë£Œ ì™„ì „ í™œìš© ìµìŠ¤íŠ¸ë¦¼ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ. íŒŒì¼ ì €ì¥ ì‹œì‘...",
        }

    def _prepare_extensive_db_context(self, documents):
        """
        ì°¸ê³  ë¬¸ì„œë“¤ì„ êµ¬ì¡°ì ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        context_parts = []

        # íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
        file_groups = {}
        for doc in documents:
            file_path = doc.get("file_path", "ë¯¸ìƒ")
            if file_path not in file_groups:
                file_groups[file_path] = []
            file_groups[file_path].append(doc)

        context_parts.append("### ğŸ“‹ ë¬¸ì„œë³„ ì •ë¦¬ëœ ì›ë¬¸ ìë£Œ\n")

        for file_path, file_docs in file_groups.items():
            context_parts.append(f"\n#### ğŸ“„ {file_path}\n")

            # í—¤ë”ë³„ë¡œ ê·¸ë£¹í™”
            header_groups = {}
            for doc in file_docs:
                headers_key = " > ".join(doc.get("headers", ["ê¸°íƒ€"]))
                if headers_key not in header_groups:
                    header_groups[headers_key] = []
                header_groups[headers_key].append(doc)

            for headers_key, header_docs in header_groups.items():
                if headers_key and headers_key != "ê¸°íƒ€":
                    context_parts.append(f"\n**ì„¹ì…˜**: {headers_key}\n")

                for i, doc in enumerate(header_docs, 1):
                    sentence = doc.get("sentence", "")
                    reference = doc.get("reference_text", "")

                    context_parts.append(f"{i}. {sentence}")
                    if reference:
                        context_parts.append(f"   ğŸ“š ì¶œì²˜: {reference}")
                    context_parts.append("")

        # ì°¸ê³ ë¬¸í—Œì´ ìˆëŠ” ì¤‘ìš” ë¬¸ì„œë“¤ ë³„ë„ ì •ë¦¬
        ref_docs = [doc for doc in documents if doc.get("reference_text")]
        if ref_docs:
            context_parts.append("\n### ğŸ¯ ì¤‘ìš” ì°¸ê³ ë¬¸í—Œ í¬í•¨ ìë£Œ\n")
            for i, doc in enumerate(ref_docs[:20], 1):  # ìƒìœ„ 20ê°œë§Œ
                context_parts.append(f"{i}. **ë‚´ìš©**: {doc.get('sentence', '')}")
                context_parts.append(f"   **ì¶œì²˜**: {doc.get('reference_text', '')}")
                context_parts.append(f"   **íŒŒì¼**: {doc.get('file_path', '')}")
                if doc.get("headers"):
                    context_parts.append(f"   **ì„¹ì…˜**: {' > '.join(doc['headers'])}")
                context_parts.append("")

        return "\n".join(context_parts)

    def node_save_bypassed_report(self, state: ReportState):
        """
        Extreme ëª¨ë“œ: ìƒì„±ëœ ë³´ê³ ì„œë¥¼ ì €ì¥í•˜ê³  ì£¼ìš” ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
        """
        self.logger.set_current_node("save_bypassed_report")
        self.logger.add_log("INFO", "[2/2] ìµìŠ¤íŠ¸ë¦¼ ë³´ê³ ì„œ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡ ì‹œì‘")
        self._update_progress("[2/2] ìµìŠ¤íŠ¸ë¦¼ ë³´ê³ ì„œ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡ ì‹œì‘...")

        # Save node-specific logs
        for node_name, history in self.logger.node_logs.items():
            log_path = os.path.join(
                self.logs_folder, f"node_{node_name}_log_{self.logger.session_id}.md"
            )
            with open(log_path, "w", encoding="utf-8") as f:
                f.write(f"# Log for node: {node_name}\n\n")
                f.write("\n".join(map(str, history)))

        self.logger.add_log("SUCCESS", "ìµìŠ¤íŠ¸ë¦¼ ëª¨ë“œ ë¡œê·¸ ì €ì¥ ì™„ë£Œ.")

        return {
            "final_report_with_refs": state[
                "final_report_with_refs"
            ],  # ì´ì „ ìƒíƒœì—ì„œ ì „ë‹¬
            "progress_message": "2/2: ìµìŠ¤íŠ¸ë¦¼ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ.",
        }

    def node_generate_outline(self, state: ReportState):
        self.logger.set_current_node("generate_outline")

        # Determine current step based on the graph being run
        total_steps = 2 if "bypass" in self.logger.session_id else 6
        progress_message_start = f"[1/{total_steps}] ë³´ê³ ì„œ ê°œìš” ìƒì„± ì‹œì‘..."
        progress_message_end = f"1/{total_steps}: ê°œìš” ìƒì„± ì™„ë£Œ. {'ìµìŠ¤íŠ¸ë¦¼ ë³´ê³ ì„œ ìƒì„±' if total_steps == 2 else 'ì´ˆì•ˆ ì‘ì„±'} ì‹œì‘..."

        self.logger.add_log("INFO", progress_message_start)
        self._update_progress(progress_message_start)

        outline = self._generate_outline_logic(state["topic"])
        self.logger.add_log("SUCCESS", f"ê°œìš” ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(outline)}ì)")

        return {
            "outline": outline,
            "progress_message": progress_message_end,
        }

    def node_generate_draft(self, state: ReportState):
        self.logger.set_current_node("generate_draft")
        self.logger.add_log("INFO", "[2/6] ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì‹œì‘")
        self._update_progress("[2/6] ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì‹œì‘...")

        outline = state["outline"]
        topic = state["topic"]

        headers = re.findall(r"^(##+)\s+(.*)", outline, re.MULTILINE)

        report_content = {}
        full_draft_parts = []
        total_headers = len(headers)

        for i, (level, header) in enumerate(headers):
            # ëª©ì°¨ì˜ ë ˆë²¨ì— ë”°ë¼ í—¤ë” ì¶”ê°€
            full_draft_parts.append(f"\n{level} {header}\n")

            progress_msg = f"[2/6] ì´ˆì•ˆ ì‘ì„± ì¤‘... ({i+1}/{total_headers}): {header}"
            self._update_progress(progress_msg)
            self.logger.add_log("INFO", progress_msg)

            section_content = self._generate_single_section(header, topic, outline)
            report_content[header] = section_content
            full_draft_parts.append(section_content)

        full_draft = "\n\n".join(full_draft_parts)

        self.logger.add_log(
            "SUCCESS", f"ë³´ê³ ì„œ ì´ˆì•ˆ ìƒì„± ì™„ë£Œ (ì´ ê¸¸ì´: {len(full_draft)}ì)"
        )
        return {
            "report_content": report_content,
            "current_report_text": full_draft,
            "progress_message": "2/6: ì´ˆì•ˆ ìƒì„± ì™„ë£Œ. í¸ì§‘ì¥ ê²€í†  ì‹œì‘...",
        }

    def node_editorial_review(self, state: ReportState):
        self.logger.set_current_node("editorial_review")
        self.logger.add_log("INFO", "[3/6] í¸ì§‘ì¥ ê²€í†  ë° ê°œì„  ì‘ì—… ì‹œì‘")
        review_attempts = state["review_attempts"] + 1
        self._update_progress(
            f"í¸ì§‘ì¥ ê²€í† ... (ì‹œë„ {review_attempts}/{config.MAX_REVIEW_ATTEMPTS})"
        )

        report_text = state["current_report_text"]

        # 1. ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        char_count = len(report_text)
        word_count = len(report_text.split())
        # ê°ì£¼ ê°œìˆ˜ ê³„ì‚° (ìœ ì—°í•œ ì •ê·œì‹ ì‚¬ìš©)
        ref_count = len(re.findall(r"[(\[]\^?([\w-]+)[)\]]", report_text))
        model_name = self.models.get("editorial_review", "N/A")

        # 2. ê²€í†  ë¡œì§ ì‹¤í–‰
        review_result = self._editorial_review_logic(report_text, state["outline"])

        # 3. ë©”íƒ€ë°ì´í„°ë¥¼ ê²€í†  ê²°ê³¼ì— ì¶”ê°€
        review_result["metadata"] = {
            "model_used": model_name,
            "char_count": char_count,
            "word_count": word_count,
            "reference_count": ref_count,
        }

        self.logger.add_log(
            "INFO",
            f"ê²€í†  ê²°ê³¼: í†µê³¼={review_result.get('review_passed')}, ê°œì„  í•„ìš”={len(review_result.get('sections_to_improve', []))}ê°œ",
        )
        self.logger.add_log(
            "DEBUG", f"ê²€í†  ì´í‰: {review_result.get('overall_comment')}"
        )

        return {
            "review_result": review_result,
            "review_attempts": review_attempts,
            "review_history": state["review_history"] + [review_result],
            "progress_message": "3/6: í¸ì§‘ì¥ ê²€í†  ì™„ë£Œ. í•„ìš”ì‹œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.",
        }

    def node_regenerate_sections(self, state: ReportState):
        self.logger.set_current_node("regenerate_sections")
        sections_to_improve = state["review_result"].get("sections_to_improve", [])
        self.logger.add_log(
            "INFO", f"ë³´ê³ ì„œ ì¼ë¶€ ì¬ì‘ì„± ì‹œì‘ ({len(sections_to_improve)}ê°œ ì„¹ì…˜)"
        )
        self._update_progress(
            f"ë³´ê³ ì„œ ì¼ë¶€ ì¬ì‘ì„± ì¤‘... ({len(sections_to_improve)}ê°œ ì„¹ì…˜)"
        )

        updated_content = state["report_content"].copy()

        for i, section in enumerate(sections_to_improve):
            header = section["section_header"]
            instructions = section["improvement_instruction"]

            progress_msg = f"ì¬ì‘ì„± ì¤‘... ({i+1}/{len(sections_to_improve)}): {header}"
            self._update_progress(progress_msg)
            self.logger.add_log("INFO", progress_msg)
            self.logger.add_log("DEBUG", f"ê°œì„  ì§€ì‹œì‚¬í•­: {instructions}")

            if header in updated_content:
                regenerated_content = self._generate_single_section(
                    header, state["topic"], state["outline"], instructions
                )
                updated_content[header] = regenerated_content
            else:
                self.logger.add_log(
                    "WARN", f"ì¬ì‘ì„±í•  ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {header}"
                )

        # ì¬ì‘ì„±ëœ ë‚´ìš©ìœ¼ë¡œ ì „ì²´ ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
        outline = state["outline"]
        headers_in_outline = re.findall(r"^(##+)\s+(.*)", outline, re.MULTILINE)
        new_report_parts = []
        for level, h in headers_in_outline:
            new_report_parts.append(f"\n{level} {h}\n")
            new_report_parts.append(updated_content.get(h, ""))

        new_report_text = "\n\n".join(new_report_parts)

        self.logger.add_log("SUCCESS", "ë³´ê³ ì„œ ì¼ë¶€ ì¬ì‘ì„± ì™„ë£Œ.")
        return {
            "report_content": updated_content,
            "current_report_text": new_report_text,
            "progress_message": "ì¼ë¶€ ì¬ì‘ì„± ì™„ë£Œ. ë‹¤ì‹œ í¸ì§‘ì¥ ê²€í† ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.",
        }

    def node_final_formatting(self, state: ReportState):
        self.logger.set_current_node("final_formatting")
        self.logger.add_log("INFO", "[4/6] ìµœì¢… ì„œì‹ ì •ë¦¬ ì‹œì‘")
        self._update_progress("[4/6] ìµœì¢… ì„œì‹ ì •ë¦¬ ì‹œì‘...")

        original_text = state["current_report_text"]

        # 1. (ë³´í˜¸) ê°€ëŠ¥í•œ ëª¨ë“  í˜•íƒœì˜ ê°ì£¼ë¥¼ 'ìˆœì„œëŒ€ë¡œ' ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.
        # ì •ê·œì‹ì€ [^uuid], [uuid], (uuid), (^uuid) ë“±ì„ ëª¨ë‘ í¬ê´„í•©ë‹ˆë‹¤.
        original_citations_matches = list(
            re.finditer(r"[(\[]\^?([\w-]+)[)\]]", original_text)
        )

        if not original_citations_matches:
            self.logger.add_log(
                "WARN", "ë³´í˜¸í•  ê°ì£¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„œì‹ ì •ë¦¬ë§Œ ì§„í–‰í•©ë‹ˆë‹¤."
            )
            formatted_report = self._final_formatting_logic(original_text)
            return {
                "formatted_report": formatted_report,
                "progress_message": "4/6: ì„œì‹ ì •ë¦¬ ì™„ë£Œ. ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ì‹œì‘...",
            }

        # 2. ë³¸ë¬¸ì˜ ê°ì£¼ë¥¼ [ì°¸ê³ ë¬¸í—Œ:ì¸ë±ìŠ¤] í˜•íƒœë¡œ ìˆœì„œëŒ€ë¡œ ë°”ê¿‰ë‹ˆë‹¤.
        citation_counter = 0

        def protect_placeholder(match):
            nonlocal citation_counter
            placeholder = f"[ì°¸ê³ ë¬¸í—Œ:{citation_counter}]"
            citation_counter += 1
            return placeholder

        protected_text = re.sub(
            r"[(\[]\^?([\w-]+)[)\]]", protect_placeholder, original_text
        )
        self.logger.add_log(
            "DEBUG",
            f"{len(original_citations_matches)}ê°œì˜ ê°ì£¼ë¥¼ ì°¾ì•„ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ êµì²´í–ˆìŠµë‹ˆë‹¤.",
        )

        # 3. ë³´í˜¸ëœ í…ìŠ¤íŠ¸ë¡œ ì„œì‹ ì •ë¦¬ ë¡œì§(LLM í˜¸ì¶œ)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        formatted_protected_text = self._final_formatting_logic(protected_text)

        # 4. (ë³µì›) [ì°¸ê³ ë¬¸í—Œ:ì¸ë±ìŠ¤]ë¥¼ ì›ë˜ì˜ í‘œì¤€ í˜•ì‹ [^uuid]ë¡œ ë˜ëŒë¦½ë‹ˆë‹¤.
        restored_text = formatted_protected_text

        # ë³µì›ì„ ìœ„í•´ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ë‹¤ì‹œ ì°¾ìŠµë‹ˆë‹¤.
        temp_placeholders_in_formatted_text = re.findall(
            r"\[ì°¸ê³ ë¬¸í—Œ:(\d+)\]", restored_text
        )

        num_restored = 0
        for index_str in temp_placeholders_in_formatted_text:
            index = int(index_str)
            if index < len(original_citations_matches):
                placeholder_to_replace = f"[ì°¸ê³ ë¬¸í—Œ:{index}]"
                # ì›ë³¸ uuidë¥¼ ê°€ì ¸ì™€ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³µì›
                original_uuid = original_citations_matches[index].group(1)
                standard_citation = f"[^{original_uuid}]"

                # ì¤‘ë³µ êµì²´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ 1ë²ˆë§Œ êµì²´
                if placeholder_to_replace in restored_text:
                    restored_text = restored_text.replace(
                        placeholder_to_replace, standard_citation, 1
                    )
                    num_restored += 1

        self.logger.add_log(
            "DEBUG",
            f"{num_restored}/{len(original_citations_matches)}ê°œì˜ ê°ì£¼ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤.",
        )

        self.logger.add_log(
            "SUCCESS", f"ìµœì¢… ì„œì‹ ì •ë¦¬ ì™„ë£Œ (ê¸¸ì´: {len(restored_text)}ì)"
        )
        return {
            "formatted_report": restored_text,
            "progress_message": "4/6: ì„œì‹ ì •ë¦¬ ì™„ë£Œ. ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ì‹œì‘...",
        }

    def node_finalize_citations_and_save_log(self, state: ReportState):
        self.logger.set_current_node("finalize_and_save")
        self.logger.add_log("INFO", "[5/6] ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ë° ë¡œê·¸ ì €ì¥ ì‹œì‘")
        self._update_progress("[5/6] ìµœì¢… ê°ì£¼ ì²˜ë¦¬ ë° ë¡œê·¸ ì €ì¥ ì‹œì‘...")
        final_report_with_refs = self._finalize_citations_logic(
            state["formatted_report"]
        )
        self.logger.add_log(
            "SUCCESS",
            f"ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ (ì°¸ê³ ë¬¸í—Œ í¬í•¨, ê¸¸ì´: {len(final_report_with_refs)}ì)",
        )

        # Save node-specific logs
        for node_name, history in self.logger.node_logs.items():
            log_path = os.path.join(
                self.logs_folder, f"node_{node_name}_log_{self.logger.session_id}.md"
            )
            with open(log_path, "w", encoding="utf-8") as f:
                f.write(f"# Log for node: {node_name}\n\n")
                # historyì˜ ê° ì•„ì´í…œì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ join
                f.write("\n".join(map(str, history)))

        # Save editorial review history
        review_log_path = os.path.join(
            self.results_folder, f"editorial_review_log_{self.logger.session_id}.md"
        )
        with open(review_log_path, "w", encoding="utf-8") as f:
            f.write("# Editorial Review History\n\n")
            for i, review in enumerate(state["review_history"]):
                f.write(f"## Attempt {i+1}\n\n")

                # ë©”íƒ€ë°ì´í„° ê¸°ë¡
                metadata = review.get("metadata", {})
                if metadata:
                    f.write("### Review Metadata\n\n")
                    f.write(
                        f"- **Model Used:** `{metadata.get('model_used', 'N/A')}`\n"
                    )
                    f.write(
                        f"- **Character Count:** {metadata.get('char_count', 0):,}\n"
                    )
                    f.write(f"- **Word Count:** {metadata.get('word_count', 0):,}\n")
                    f.write(
                        f"- **Reference Count:** {metadata.get('reference_count', 0)}\n\n"
                    )

                f.write("### Review Result\n\n")
                f.write(f"**Passed:** {review.get('review_passed')}\n\n")
                f.write(
                    f"**Overall Comment:**\n\n```\n{review.get('overall_comment')}\n```\n\n"
                )

                if review.get("sections_to_improve"):
                    f.write("### Sections to Improve\n\n")
                    for section in review.get("sections_to_improve"):
                        f.write(
                            f"- **{section['section_header']}**: {section['improvement_instruction']}\n"
                        )

                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‘ë‹µ ê¸°ë¡
                if "raw_response" in review:
                    f.write("\n### Raw Model Response (on parsing failure)\n\n")
                    f.write(f"```\n{review['raw_response']}\n```\n\n")

                f.write("\n---\n\n")

        return {
            "final_report_with_refs": final_report_with_refs,
            "progress_message": "5/6: ê°ì£¼ ì²˜ë¦¬ ë° ë¡œê·¸ ì €ì¥ ì™„ë£Œ.",
        }

    def should_continue_review(self, state: ReportState):
        review_result = state["review_result"]
        review_attempts = state["review_attempts"]

        if review_result.get("review_passed"):
            return "end_review"

        if review_attempts >= config.MAX_REVIEW_ATTEMPTS:
            self.logger.add_log(
                "WARN",
                f"ìµœëŒ€ ê²€í†  íšŸìˆ˜({config.MAX_REVIEW_ATTEMPTS})ì— ë„ë‹¬í•˜ì—¬, ê°œì„  ì—†ì´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.",
            )
            return "end_review"

        if not review_result.get("sections_to_improve"):
            self.logger.add_log(
                "WARN",
                "ê²€í† ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ê°œì„ í•  ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ì¬ê²€í† ë¥¼ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.",
            )

        return "regenerate"

    def _build_graph(self):
        workflow = StateGraph(ReportState)
        workflow.add_node("generate_outline", self.node_generate_outline)
        workflow.add_node("generate_draft", self.node_generate_draft)
        workflow.add_node("editorial_review", self.node_editorial_review)
        workflow.add_node("regenerate_sections", self.node_regenerate_sections)
        workflow.add_node("final_formatting", self.node_final_formatting)
        workflow.add_node(
            "finalize_and_save", self.node_finalize_citations_and_save_log
        )
        workflow.set_entry_point("generate_outline")
        workflow.add_edge("generate_outline", "generate_draft")
        workflow.add_edge("generate_draft", "editorial_review")
        workflow.add_conditional_edges(
            "editorial_review",
            self.should_continue_review,
            {"regenerate": "regenerate_sections", "end_review": "final_formatting"},
        )
        workflow.add_edge("regenerate_sections", "editorial_review")
        workflow.add_edge("final_formatting", "finalize_and_save")
        workflow.add_edge("finalize_and_save", END)
        return workflow.compile()

    def _build_bypass_graph(self):
        workflow = StateGraph(ReportState)
        workflow.add_node("generate_outline", self.node_generate_outline)
        workflow.add_node(
            "generate_bypassed_report", self.node_generate_bypassed_report
        )
        workflow.add_node("save_bypassed_report", self.node_save_bypassed_report)

        workflow.set_entry_point("generate_outline")
        workflow.add_edge("generate_outline", "generate_bypassed_report")
        workflow.add_edge("generate_bypassed_report", "save_bypassed_report")
        workflow.add_edge("save_bypassed_report", END)

        return workflow.compile()
